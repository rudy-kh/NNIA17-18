{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Regularization - Bagging, Early Stopping and Dropout (deadline: 22 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Regularization: Bagging (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **bagging** regularization on Decision Tree based methods against a single instance of such a classifier.\n",
    "\n",
    "Bagging, briefly mentioned in the Lecture 6, refers to an ensemble machine learning method. The Bagging scheme, suggested to be used in this exercise, samples instances from the training data with replacement and creates multiple training subsets. For each of these subsets, a new regressor is constructed internally and finally, all combined to produce the result. For more details read: \n",
    "\n",
    "1. Scikit Learn Documentation for Bagging Regressor. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#id6\n",
    "\n",
    "2. Bootstrap Aggregating Wikipedia Article. https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "Implement a bagging regularization scheme using ***DecisionTreeRegressor***, a Decision Tree based classifier from the python package ***sklearn.tree***. To implement the bagging scheme you can use ***BaggingRegressor*** available in the python package ***sklearn.ensemble***. Fill in the code pieces marked by \"# TODO\" in the following notebook to complete this assignment. Finally, comment on the results you obtain.\n",
    "\n",
    "Note: to run the following code you will need to download **data.csv** from the NNIA's resource page on Piazza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Estimators: Create an array of two estimators.\n",
    "# First a \"Tree\" using \"DecisionTreeRegressor\" and second a Regularized version obtain using \"BaggingRegressor\"\n",
    "# on this Tree, labelled \"Bagging (Tree)\". (2 points)\n",
    "\n",
    "estimators = [(\"Tree\", #TODO\n",
    "               DecisionTreeRegressor()\n",
    "              ),\n",
    "              (\"Bagging (Tree)\", #TODO\n",
    "               BaggingRegressor(base_estimator =DecisionTreeRegressor() ,max_samples=1.0,\n",
    "                                bootstrap=True)\n",
    "              )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = len(estimators)\n",
    "np.random.seed(0)\n",
    "\n",
    "#Load Data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "#Drop Player Field\n",
    "data = data.drop('Player', axis=1)\n",
    "\n",
    "#Set variable y with 'Salary' column and then drop it from data\n",
    "y = data['Salary'].as_matrix()\n",
    "data = data.drop('Salary', axis=1)\n",
    "\n",
    "#Convert data to an numpy array x\n",
    "x = data.as_matrix()\n",
    "\n",
    "# Split x in X_train (80 %) and X_test (20 %),  while constructing the corresponding y_train and y_test\n",
    "n_train = np.int(0.8 * len(x) )\n",
    "n_test = len(x) - n_train\n",
    "X_train = x[:n_train,:]\n",
    "y_train = y[:n_train]\n",
    "X_test = x[n_train:,:]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Tree: 152196.4062 (error)\n",
      "Bagging (Tree): 64003.2045 (error)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAADSCAYAAADOrS1SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNCku5BzHcRzHcZoUF3KO4ziO4zhNyv8HWT63t7nt3KsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2361dc65a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Figures and report error using the different ensemble methods\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "print(n_estimators)\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    #print (n, name, estimator) n ={0,1}, name = {Tree, Bagging (Tree)}, estimator is the function\n",
    "    \n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros(n_test)\n",
    "    \n",
    "    # Train the estimator (1 point)\n",
    "    # TODO\n",
    "    estimator.fit(X_train, y_train)\n",
    "    #Predict results using the estimator on X_test (1 point)\n",
    "    # TODO\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    y_error = np.zeros(n_test)\n",
    "    \n",
    "    # Compute the sqaured error using y_test and y_predict and store it in y_error (1 point)\n",
    "    # TODO\n",
    "    for i in range(y_test.size):\n",
    "        y_error[i] = (y_test[i] - y_predict[i])**2\n",
    "    \n",
    "    print(\"{0}: {1:.4f} (error)\".format(name,np.mean(y_error)))\n",
    "    \n",
    "    # Plot the Result\n",
    "    plt.subplot(1,n_estimators, n+1)\n",
    "    plt.plot(np.arange(n_test), y_error, \"r\", label=\"$error(x)$\")\n",
    "    plt.ylim([0, 1300000])\n",
    "    plt.title(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Explain the differences (in 2-3 sentences) between the plots you obtain for **Tree** and **Bagging (Tree)**. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging trains multiple decision trees, each on a different sample generated from the training data. Then Bagging would average all the predictions, which yields to output a lower error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Regularization: Early Stopping (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** To study how increasing neurons of a neural network (model complexity) affects the Early Stopping threshold.\n",
    "\n",
    "Download **MNIST** dataset from the NNIA's resource page on Piazza. We first update the feedforward neural network code below (at \"#TODO\") to calculate training and validation error at every 100 iterations of the training scheme. Then using this code for a different number of neurons (50 and 200), plot the variation of training and validation error for every 100th iteration up to 700 iterations. Studying these plots answer the questions given below.\n",
    "\n",
    "Note: to calculate the validation error use the test set as a proxy validation set. Also, notice the extra arguments that need to be provided to calculate the validation error at every 100 iterations in the fit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from __future__ import division\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "        \n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "        \n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=1):\n",
    "\n",
    "        self.n_output = n_output\n",
    "        self.r = np.random.RandomState(random_state)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "\n",
    "    def _encode_labels(self, y, k):    \n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \n",
    "        w1 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        \n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        \n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, validation_freq=0, X_val=None, y_val=None):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "        validation_freq : int (default: 0)\n",
    "            For the value \"i\" it takes, it calculates the \n",
    "            train set and validation set error every \"ith\" iteration\n",
    "        X_val : array, shape = [n_validation_samples, n_features]\n",
    "            the validation set X values, to be provided \n",
    "            when validation_freq > 0\n",
    "        y_val : array, shape = [n_validation_samples]\n",
    "            the validation set y values, to be provided\n",
    "            when validation_freq > 0\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cost_ = []\n",
    "        self.train_err_ = []\n",
    "        self.val_err_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "        epoch_strlen = len(str(self.epochs))  # for progress formatting\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "            \n",
    "            # Implement a code block to check the training and validation error (in percentage) for every given number\n",
    "            # of iterations stored in validation frequency. The training error is calculated on input X and validation\n",
    "            # error on input X_val. Store the training and validation error in self.train_err_ and self.val_err_            # \n",
    "            # respectively. This will help plotting the errors in the following code (2 points)\n",
    "            # TODO\n",
    "            if self.shuffle:\n",
    "                idx = self.r.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "            \n",
    "            # evaluation after validation_freq iteration \n",
    "            if ((i + 1) % validation_freq == 0 ):\n",
    "        \n",
    "                y_train_pred = self.predict(X_train)\n",
    "                y_val_pred = self.predict(X_val)\n",
    "\n",
    "                train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /\n",
    "                             X_train.shape[0])\n",
    "                val_acc = ((np.sum(y_val == y_val_pred)).astype(np.float) /\n",
    "                             X_val.shape[0])\n",
    "\n",
    "                train_error = 1 - train_acc\n",
    "                val_error = 1 - val_acc\n",
    "                print('\\r%0*d/%d | Train/Valid error: %.2f%%/%.2f%% ' %\n",
    "                                 (epoch_strlen, i+1, self.epochs,\n",
    "                                  train_error*100, val_error*100))\n",
    "                \n",
    "\n",
    "                self.train_err_.append(train_error)\n",
    "                self.val_err_.append(val_error)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, columns: 784\n",
      "Rows: 10000, columns: 784\n",
      "100/700 | Train/Valid error: 11.22%/11.66% \n",
      "200/700 | Train/Valid error: 9.58%/10.48% \n",
      "300/700 | Train/Valid error: 8.57%/10.15% \n",
      "400/700 | Train/Valid error: 7.36%/9.02% \n",
      "500/700 | Train/Valid error: 6.64%/8.64% \n",
      "600/700 | Train/Valid error: 6.10%/8.48% \n",
      "700/700 | Train/Valid error: 5.96%/8.56% \n",
      "Train/Valid error: 5.96%/8.56% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rudy-kh\\Miniconda3\\envs\\nnia\\lib\\site-packages\\matplotlib\\axes\\_base.py:3245: UserWarning: Attempted to set non-positive ylimits for log-scale axis; invalid limits will be ignored.\n",
      "  'Attempted to set non-positive ylimits for log-scale axis; '\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT0AAADgCAYAAABxTLHYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVPMMDi2aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2361f35ee48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/700 | Train/Valid error: 7.06%/9.25% \n",
      "200/700 | Train/Valid error: 5.11%/7.77% \n",
      "300/700 | Train/Valid error: 3.60%/6.80% \n",
      "400/700 | Train/Valid error: 2.61%/6.40% \n",
      "500/700 | Train/Valid error: 2.29%/6.46% \n",
      "600/700 | Train/Valid error: 2.03%/6.36% \n",
      "700/700 | Train/Valid error: 1.95%/6.42% \n",
      "Train/Valid error: 1.95%/6.42% \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANcAAAEWCAYAAAD1vgIQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYjzU8mAkgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2361fc2ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load train and test data\n",
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "r = np.random.RandomState(1)\n",
    "idx = r.choice(y_train.shape[0],10000,replace=False)\n",
    "X_train, y_train = X_train[idx], y_train[idx]\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "    \n",
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "neurons = [50, 200]\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "for n, hidden in enumerate(neurons):\n",
    "    #create a MLP object\n",
    "    nn = MLP(n_output=10, \n",
    "             n_features=X_train.shape[1], \n",
    "             n_hidden=hidden, \n",
    "             l2=0.0, \n",
    "             l1=0.0, \n",
    "             epochs=700, \n",
    "             eta=0.001,\n",
    "             alpha=0.001,\n",
    "             decrease_const=0.00001,\n",
    "             minibatches=50, \n",
    "             shuffle=False,\n",
    "             random_state=1)\n",
    "    \n",
    "    # Fit the data with NN\n",
    "    nn.fit(X_train, y_train, print_progress=False,\n",
    "             validation_freq=100,\n",
    "             X_val=X_test, y_val=y_test)\n",
    "    \n",
    "    iter = [ i*100 for i in range(700 // 100)]\n",
    "    plt.subplot(1,len(neurons),n+1)\n",
    "    plt.plot(iter, nn.train_err_)\n",
    "    plt.plot(iter, nn.val_err_)\n",
    "    plt.yscale('log')\n",
    "    plt.ylim([0, 20])\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title(\"Neurons: {0} (error)\".format(hidden))\n",
    "    \n",
    "    # Print the training accuracy and test accuracy in percentage for each value of hidden layer size. (0.5 point) \n",
    "    # TODO\n",
    "    y_train_pred = nn.predict(X_train)\n",
    "    y_val_pred = nn.predict(X_test)\n",
    "\n",
    "    train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
    "    \n",
    "    val_acc = ((np.sum(y_test == y_val_pred)).astype(np.float) / X_test.shape[0])\n",
    "    \n",
    "    train_error = 1 - train_acc\n",
    "    val_error = 1 - val_acc\n",
    "    print('Train/Valid error: %.2f%%/%.2f%% ' % (train_error*100, val_error*100))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Using these plots and the related variables from the code to make suggestions for an early stopping criteria for each hidden layer size. (1.5 points)\n",
    "\n",
    "##### We can see from the both plots and the validation error that for 50 neurons a validation_freq of 600 iterations makes since. After that, the network overfits the training data.\n",
    "\n",
    "##### For the second network (200 neurons), we can see that a validation_freq of 400 iterations makes a since, and after that the network start to overfit the data. \n",
    "\n",
    "b) As the number of neurons are increased, you will observe differences in the early stopping criteria for each hidden layer size. Why do you observe such differences? (2 points)\n",
    "\n",
    "##### As the number of neurons in the hidden layer increases, the network starts to fit the training data early and with better results, and then it overfits the training data by memorizing the training data, which would work well on the training data but will not generalize well on the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 3. Regularization: Dropout (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** To implement and study dropout for neural networks.\n",
    "\n",
    "Implement dropout for layer 2 in the three-layered network you developed for Exercise 2. A simple dropout implementation creates a mask ($r^{(l)}_j$) for every neuron $j$ of the hidden layer $l$ by drawing from a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with probability $p$.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) $$\n",
    "This mask is then applied to the hidden layer output ($h^{(l)}$) to obtain the regularized hidden layer activation $\\hat{h}^{(l)}$\n",
    "$$ \\hat{h}^{(l)} = r^{(l)} * h^{(l)}$$\n",
    "However, such an implementation requires the layerl be multiplied by the dropout coefficient $p$ at evaluation time to balance the larger number of active units during testing.\n",
    "$$ \\hat{h}^{(l)} = p * h^{(l)}$$\n",
    "Such an implementation requires the code to switch between different code blocks for forward-pass evaluation during training and testing. Hence, a smoother way to implement dropout is to use ***inverted dropout*** where the mask generated at the training is multiplied by the inverse of the dropout coefficient.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) * \\frac{1}{p}$$\n",
    "This scheme allows the scaling to be learned during training and hence, no switching between code blocks is required.\n",
    "\n",
    "Update the code below (specified by #TODO) to implement inverted dropout for a hidden layer size of 50 neurons. Running the code will show you variation of training and test accuracy for the dropout values given in the code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5000, columns: 784\n",
      "Rows: 5000, columns: 784\n",
      "Dropout: 0.10\n",
      "Train/Valid error : 43.22%/44.72% \n",
      "Dropout: 0.30\n",
      "Train/Valid error : 17.08%/18.66% \n",
      "Dropout: 0.50\n",
      "Train/Valid error : 10.78%/12.46% \n",
      "Dropout: 0.70\n",
      "Train/Valid error : 8.54%/10.64% \n",
      "Dropout: 0.90\n",
      "Train/Valid error : 9.00%/11.34% \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from __future__ import division\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "  \n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "        \n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "        \n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "    \n",
    "    dropout : float (default: 0.5)\n",
    "        Set the dropout coefficient\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=1, dropout = 0.5):\n",
    "\n",
    "        self.n_output = n_output\n",
    "        self.r = np.random.RandomState(random_state)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "\n",
    "        # Initialize the class variable \"dropout\" like other variables above. Also, initialize a variable mask to None.\n",
    "        # This will allow sharing dropout information during forward and backward pass of the neural networks. Note \n",
    "        # that the __init__ function has already been modified to include dropout coefficient as an argument. (0.5 points)\n",
    "        #TODO\n",
    "        self.dropout = dropout\n",
    "        self.mask = None\n",
    "\n",
    "    def _encode_labels(self, y, k):    \n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \n",
    "        w1 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        \n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        \n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        \n",
    "        # Implement inverted dropout using class variables dropout and activation variable (a2) for the forward\n",
    "        # pass for the second hidden layer below. To create the mask you will have to use self.r.binomial for \n",
    "        # generating the bernoulli distribution. The mask created here needs to be stored in the appropriate mask\n",
    "        # variable defined in the __init__ function for further use by the backward pass. (2.5 points)\n",
    "        #TODO\n",
    "        \n",
    "        self.mask = np.random.binomial(1, self.dropout, a2.shape) / self.dropout\n",
    "\n",
    "        masked_a2 = self.mask * a2\n",
    "\n",
    "\n",
    "        z3 = w2.dot(masked_a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        np.seterr(divide='ignore')\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        \n",
    "        term1[np.isneginf(term1)] = 0\n",
    "        term2[np.isneginf(term2)] = 0\n",
    "        \n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, z3, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        sigma3 = (a3 - y_enc) #* self._sigmoid_gradient(z3)\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        \n",
    "        # Implement dropout for the backward pass, use class variables for mask and dropout for this task (2.5 points)\n",
    "        # TODO\n",
    "        \n",
    "        masked_a2 = a2 * self.mask\n",
    "        sigma2 = sigma2 * self.mask\n",
    "\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(masked_a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2, z3=z3,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "# Load train and test data\n",
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "r = np.random.RandomState(1)\n",
    "idx = r.choice(y_train.shape[0],5000,replace=False)\n",
    "X_train, y_train = X_train[idx], y_train[idx]\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "    \n",
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "idx = r.choice(y_test.shape[0],5000,replace=False)\n",
    "X_test, y_test = X_test[idx], y_test[idx]\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "  \n",
    "vals = np.arange(0.1, 1.0, 0.2)\n",
    "for dropout in vals:\n",
    "    # create a MLP object\n",
    "    print('Dropout: %.2f' % dropout)\n",
    "    nn = MLP(n_output=10, \n",
    "             n_features=X_train.shape[1], \n",
    "             n_hidden=50, \n",
    "             l2=0.0, \n",
    "             l1=0.0, \n",
    "             epochs=500, \n",
    "             eta=0.001,\n",
    "             alpha=0.001,\n",
    "             decrease_const=0.00001,\n",
    "             minibatches=50, \n",
    "             shuffle=False,\n",
    "             random_state=1,\n",
    "             dropout = dropout)\n",
    "    \n",
    "    # Fit the data with NN\n",
    "    nn.fit(X_train, y_train, print_progress=False)\n",
    "    \n",
    "    # Print the training accuracy and test accuracy in percentage for each value of dropout. This part is same as \n",
    "    # a question asked in Exercise 2, so you may use the code from there. (0 points) \n",
    "    # TODO\n",
    "    nn.dropout = 1\n",
    "    prediction_train = nn.predict(X_train)\n",
    "    training_accuracy = np.sum(prediction_train == y_train, axis = 0) / X_train.shape[0]\n",
    "    prediction_test = nn.predict(X_test)\n",
    "    test_accuracy = np.sum(prediction_test == y_test, axis = 0) / X_test.shape[0]\n",
    "    train_error = 1 - training_accuracy\n",
    "    val_error = 1 - test_accuracy\n",
    "    print('Train/Valid error : %.2f%%/%.2f%% ' % (train_error*100, val_error*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Intuitively, L1 and L2 minimize the interdependence and the value of feature weights by penalising the loss function. In the same vein, what kind of interdependence does dropout affect? (0.5 points)\n",
    "\n",
    "##### Dropout helps reducing interdependent learning between the units (neurons).\n",
    "\n",
    "b) Why can Dropout be considered as an approximation to Bagging? (1 point)\n",
    "\n",
    "##### As we have seen in the 1st question, Bagging train multiple learners (Decision Trees) on different samples generated from the training data which allows to learn several independent representations of the patterns in the data. Dropout works the same, by training multiple networks (different neworks are trained due to ignoring some randomly neurons during training) which make each network also learn a different independent representation of the patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as a solution. The naming should include the assignment number and matriculation IDs of all team members in the following format:\n",
    "**assignment-7_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 team members). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please, submit your solution to your tutor (with **[NNIA][assignment-7]** in email subject):\n",
    "1. Maksym Andriushchenko s8mmandr@stud.uni-saarland.de\n",
    "2. Marius Mosbach s9msmosb@stud.uni-saarland.de\n",
    "3. Rajarshi Biswas rbisw17@gmail.com\n",
    "4. Marimuthu Kalimuthu s8makali@stud.uni-saarland.de\n",
    "\n",
    "**If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

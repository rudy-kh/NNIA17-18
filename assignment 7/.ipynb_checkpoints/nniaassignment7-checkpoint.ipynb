{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Regularization - Bagging, Early Stopping and Dropout (deadline: 22 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Regularization: Bagging (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **bagging** regularization on Decision Tree based methods against a single instance of such a classifier.\n",
    "\n",
    "Bagging, briefly mentioned in the Lecture 6, refers to an ensemble machine learning method. The Bagging scheme, suggested to be used in this exercise, samples instances from the training data with replacement and creates multiple training subsets. For each of these subsets, a new regressor is constructed internally and finally, all combined to produce the result. For more details read: \n",
    "\n",
    "1. Scikit Learn Documentation for Bagging Regressor. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#id6\n",
    "\n",
    "2. Bootstrap Aggregating Wikipedia Article. https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "Implement a bagging regularization scheme using ***DecisionTreeRegressor***, a Decision Tree based classifier from the python package ***sklearn.tree***. To implement the bagging scheme you can use ***BaggingRegressor*** available in the python package ***sklearn.ensemble***. Fill in the code pieces marked by \"# TODO\" in the following notebook to complete this assignment. Finally, comment on the results you obtain.\n",
    "\n",
    "Note: to run the following code you will need to download **data.csv** from the NNIA's resource page on Piazza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Define Estimators`: Create an array of two estimators. <br>\n",
    "First a \"Tree\" using `DecisionTreeRegressor` and  <br>\n",
    "second a Regularized version obtain using `BaggingRegressor` on this Tree, labelled \"Bagging (Tree)\". (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = [(\"Tree\", #TODO\n",
    "               DecisionTreeRegressor()\n",
    "              ),\n",
    "              (\"Bagging (Tree)\", #TODO\n",
    "               BaggingRegressor(max_samples=1.0,\n",
    "                                bootstrap=True)\n",
    "              )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = len(estimators)\n",
    "#print (n_estimators)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "#Load Data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "#Drop Player Field\n",
    "data = data.drop('Player', axis=1)\n",
    "\n",
    "#Set variable y with 'Salary' column and then drop it from data\n",
    "y = data['Salary'].as_matrix()\n",
    "data = data.drop('Salary', axis=1)\n",
    "\n",
    "#Convert data to an numpy array x\n",
    "x = data.as_matrix()\n",
    "#print (plt.subplot(1,n_estimators,1))\n",
    "#print(plt)\n",
    "# Split x in X_train (80 %) and X_test (20 %),  while constructing the corresponding y_train and y_test\n",
    "n_train = np.int(0.8 * len(x) )\n",
    "n_test = len(x) - n_train\n",
    "X_train = x[:n_train,:]\n",
    "y_train = y[:n_train]\n",
    "X_test = x[n_train:,:]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-d727f31ce518>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-d727f31ce518>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    ax = plt.subplot(1,n_estimators, n+1)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Plot Figures and report error using the different ensemble methods\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "print(n_estimators)\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    #print (n, name, estimator) n ={0,1}, name = {Tree, Bagging (Tree)}, estimator is the function\n",
    "    \n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros(n_test)\n",
    "    \n",
    "    # Train the estimator (1 point)\n",
    "    # TODO\n",
    "    estimator.fit(X_train, y_train)\n",
    "    #Predict results using the estimator on X_test (1 point)\n",
    "    # TODO\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    y_error = np.zeros(n_test)\n",
    "    \n",
    "    # Compute the sqaured error using y_test and y_predict and store it in y_error (1 point)\n",
    "    # TODO\n",
    "    for i in range(y_test.size):\n",
    "        y_error[i] = (y_test[i] - y_predict[i])**2\n",
    "    #y_error = accuracy_score(y_test, y_predict)\n",
    "    print(\"{0}: {1:.4f} (error)\".format(name,np.mean(y_error)))\n",
    "    \n",
    "    # Plot the Result\n",
    "    plt.subplot(1,n_estimators, n+1)\n",
    "    plt.plot(np.arange(n_test), y_error, \"r\", label=\"$error(x)$\")\n",
    "    plt.ylim([0, 1300000])\n",
    "    plt.title(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Explain the differences (in 2-3 sentences) between the plots you obtain for **Tree** and **Bagging (Tree)**. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Regularization: Early Stopping (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** To study how increasing neurons of a neural network (model complexity) affects the Early Stopping threshold.\n",
    "\n",
    "Download **MNIST** dataset from the NNIA's resource page on Piazza. First, update the feedforward neural network code from  Assignment 6 to calculate training and validation error at every 100 iterations (also known as validation frequency) of the training scheme. For this update, you will modify the fit function and the signature of the function will look like follows. Notice the extra arguments that need to be provided to calculate the validation error at every 100 iterations. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def fit(self, X, y, print_progress=False, validation_freq=0, X_val=None, y_val=None):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "        validation_freq : int (default: 0)\n",
    "            For the value \"i\" it takes, it calculates the \n",
    "            train set and validation set error every \"ith\" iteration\n",
    "        X_val : array, shape = [n_validation_samples, n_features]\n",
    "            the validation set X values, to be provided \n",
    "            when validation_freq > 0\n",
    "        y_val : array, shape = [n_validation_samples]\n",
    "            the validation set y values, to be provided\n",
    "            when validation_freq > 0\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using this code for a different number of neurons (50, 100 and 200), plot the variation of training and validation error for every 100th iteration upto 1000 iterations. Label the axes and legends appropriately in the plots. (1.5 points)\n",
    "\n",
    "Note: to caluclate the validation error use the test set as a proxy validation set.\n",
    "\n",
    "a) Using these plots and the related variables from the code to make suggestions for an early stopping criteria for each hidden layer size. (1.5 points)\n",
    "\n",
    "b) As the number of neurons are increased, you will observe differences in the early stopping criteria for each hidden layer size. Why do you observe such differences? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 3. Regularization: Dropout (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** To implement and study dropout for neural networks.\n",
    "\n",
    "Implement dropout for layer 2 in the three-layered network you developed for Exercise 2. A simple dropout implementation creates a mask ($r^{(l)}_j$) for every neuron $j$ of the hidden layer $l$ by drawing from a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with probability $p$.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) $$\n",
    "This mask is then applied to the hidden layer output ($h^{(l)}$) to obtain the regularized hidden layer activation $\\hat{h}^{(l)}$\n",
    "$$ \\hat{h}^{(l)} = r^{(l)} * h^{(l)}$$\n",
    "However, such an implementation requires the layerl be multiplied by the dropout coefficient $p$ at evaluation time to balance the larger number of active units during testing.\n",
    "$$ \\hat{h}^{(l)} = p * h^{(l)}$$\n",
    "Such an implementation requires the code to switch between different code blocks for forward-pass evaluation during training and testing. Hence, a smoother way to implement dropout is to use ***inverted dropout*** where the mask generated at the training is multiplied by the inverse of the dropout coefficient.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) * \\frac{1}{p}$$\n",
    "This scheme allows the scaling to be learned during training and hence, no switching between code blocks is required.\n",
    "\n",
    "Update the code from Exercise 2 to implement inverted dropout for a hidden layer size of 200 neurons. (4 points)\n",
    "\n",
    "a) Furthermore, update the code from Exercise 2 to plot the variation of training and test accuracies on MNIST for dropout values denoted by np.arange(0.0, 1.0, 0.1). (1.5 points)\n",
    "\n",
    "b) Intuitively, L1 and L2 minimize the interdependence and the value of feature weights by penalising the loss function. In the same vein, what kind of interdependence does dropout affect? (0.5 points)\n",
    "\n",
    "c) Why can Dropout be considered as an approximation to Bagging? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as a solution. The naming should include the assignment number and matriculation IDs of all team members in the following format:\n",
    "**assignment-7_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 team members). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please, submit your solution to your tutor (with **[NNIA][assignment-7]** in email subject):\n",
    "1. Maksym Andriushchenko s8mmandr@stud.uni-saarland.de\n",
    "2. Marius Mosbach s9msmosb@stud.uni-saarland.de\n",
    "3. Rajarshi Biswas rbisw17@gmail.com\n",
    "4. Marimuthu Kalimuthu s8makali@stud.uni-saarland.de\n",
    "\n",
    "**If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

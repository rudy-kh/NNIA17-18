{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11: Sequence Learning: Recurrent and Recursive Neural Networks (deadline: 27 Jan, 23:59)\n",
    "\n",
    "For theoretical tasks you are encouraged to write in $\\\\LaTeX$. Jupyter notebooks support them by default. For reference, please have a look at the examples in this short excellent guide: [Typesetting Equations](http://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Typesetting%20Equations.ipynb)\n",
    "\n",
    "Alternatively, you can upload the solutions in the written form as images and paste them inside the cells. But if you do this, **make sure** that the images are of high quality, so that we can read them without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Comparing Vanilla RNN and LSTM for different Sequence Lengths (7 points)\n",
    "\n",
    "**Goal**: To study the variation of training performance for different sequence lengths in Vanilla RNN and Long-Short Term Memory (LSTM) Neural Networks, on a word prediction task.\n",
    "\n",
    "For this exercise, you will need to familiarize yourself with LSTMs. A good tutorial on LSTMs is presented at [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "The following LSTM tensorflow code is derived from an example [here](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/Experiments/Tensorflow/RNN/rnn_words.py). This code allows you to run an LSTM-based Neural Network on a word prediction task. The learning is set up to predict the next word given the previous `n_input` words.\n",
    "\n",
    "You will be using this code, the file `train.txt` from NNIA's resources page on Piazza and answering the following questions to complete this exercise. \n",
    "\n",
    "Note: You will need tensorflow installed to your IPython Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data...\n",
      "Iter= 1000, Average Loss= 18.179031, Average Accuracy= 5.50%\n",
      "['fawned', 'upon', 'him,', 'and', 'licked', 'his', 'hands', 'like', 'a', 'friendly'] - [dog.] vs [was]\n",
      "Iter= 2000, Average Loss= 8.306223, Average Accuracy= 6.50%\n",
      "['his', 'hands', 'like', 'a', 'friendly', 'dog.', 'the', 'emperor,', 'surprised', 'at'] - [this,] vs [to]\n",
      "Iter= 3000, Average Loss= 8.098105, Average Accuracy= 5.70%\n",
      "['his', 'victim.', 'but', 'as', 'soon', 'as', 'he', 'came', 'near', 'to'] - [androcles] vs [all]\n",
      "Iter= 4000, Average Loss= 8.001601, Average Accuracy= 5.60%\n",
      "['to', 'androcles', 'he', 'recognised', 'his', 'friend,', 'and', 'fawned', 'upon', 'him,'] - [and] vs [the]\n",
      "Iter= 5000, Average Loss= 7.931170, Average Accuracy= 4.60%\n",
      "['and', 'roaring', 'towards', 'his', 'victim.', 'but', 'as', 'soon', 'as', 'he'] - [came] vs [soon]\n",
      "Training Finished!\n",
      "Elapsed time:  32.48968482017517 sec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow..\n",
    "Next word prediction after n_input words learned from text file.\n",
    "A story is automatically generated if the predicted word is fed back as input.\n",
    "\n",
    "Source Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "# Target log path\n",
    "logs_path = '/tmp/tensorflow/rnn_words'\n",
    "writer = tf.summary.FileWriter(logs_path)\n",
    "\n",
    "# Text file containing words for training\n",
    "training_file = 'train.txt'\n",
    "\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Loaded training data...\")\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 5000\n",
    "display_step = 1000\n",
    "n_input = 10\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "    y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "    # RNN output node weights and biases\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "    }\n",
    "\n",
    "    def RNN(x, weights, biases):\n",
    "\n",
    "        # reshape to [1, n_input]\n",
    "        x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "        # Generate a n_input-element sequence of inputs\n",
    "        # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "        x = tf.split(x,n_input,1)\n",
    "\n",
    "        # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "        # TODO replace the following layer with a Vanilla RNN tf.contrib.rnn call\n",
    "        #rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "        rnn_cell = tf.contrib.rnn.BasicRNNCell(n_hidden)\n",
    "        \n",
    "        \n",
    "        # generate prediction\n",
    "        outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "        # there are n_input outputs but\n",
    "        # we only want the last output\n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "\n",
    "    pred = RNN(x, weights, biases)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Model evaluation\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Launch the Session\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        step = 0\n",
    "        offset = random.randint(0,n_input+1)\n",
    "        end_offset = n_input + 1\n",
    "        acc_total = 0\n",
    "        loss_total = 0\n",
    "\n",
    "        writer.add_graph(session.graph)\n",
    "\n",
    "        while step < training_iters:\n",
    "            # Generate a minibatch. Add some randomness on selection process.\n",
    "            if offset > (len(training_data)-end_offset):\n",
    "                offset = random.randint(0, n_input+1)\n",
    "\n",
    "            symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "            symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "            symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "            symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "            symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "            _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                    feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "            loss_total += loss\n",
    "            acc_total += acc\n",
    "            if (step+1) % display_step == 0:\n",
    "                print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                      \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "                acc_total = 0\n",
    "                loss_total = 0\n",
    "                symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "                symbols_out = training_data[offset + n_input]\n",
    "                symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "                print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "            step += 1\n",
    "            offset += (n_input+1)\n",
    "        print(\"Training Finished!\")\n",
    "        print(\"Elapsed time: \", elapsed(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) The sequence length used for prediction in the above code is specified by `n_input`. For an LSTM cell, change `n_inputs` to 3, 7, 10 and report training accuracy for each. Note: running this code on a 4GB RAM, Core i3 processor with tensorflow v1.4 takes around four to five minutes. (**1.5 points**)\n",
    "\n",
    "Ans : Average accuracy for `n_inputs = 3` after 5000 iterations is 31.30%\n",
    "\n",
    "      Average accuracy for `n_inputs = 7` after 5000 iterations is 85.30%\n",
    "      \n",
    "      Average accuracy for `n_inputs = 10` after 5000 iterations is 90.70%\n",
    "\n",
    "b) In the function `RNN`, replace the LSTM Cell with a Vanilla RNN Cell at `#TODO`. (**1 point**)\n",
    "\n",
    "c) Repeat the experiment in a) for same `n_input` values. (**1.5 points**)\n",
    "\n",
    "Ans : Average accuracy for `n_inputs = 3` after 5000 iterations is 6.60%\n",
    "\n",
    "      Average accuracy for `n_inputs = 7` after 5000 iterations is 5.30% \n",
    "      \n",
    "      Average accuracy for `n_inputs = 10` after 5000 iterations is 4.60%\n",
    "\n",
    "d) While comparing Vanilla RNN and LSTM, what trends do you observe with training accuracy when the sequence length is varied? (**1 point**)\n",
    "\n",
    "Ans : The average Accuray would go higher in LSTM model and go down in vanilla rnn when the sequence length is varied.\n",
    "\n",
    "e) Why do you think one model learns much better than the other?  (**1 point**)\n",
    "\n",
    "Ans : The difference in performance between the 2 models is due to the fact that LSTM can handle long text dependencies with varied n_input, whereas vanilla rnn can't.\n",
    "\n",
    "f) Do you expect the model with higher training accuracy to generalize well? (**1 point**)\n",
    "\n",
    "Ans : No. It could be that the model is just memorizing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Unfolding Computational Graphs. (6 points)\n",
    "\n",
    "Imagine you build a Vanilla Recurrent Neural Networks with an input layer, a Vanilla RNN based hidden layer and an output layer. \n",
    "\n",
    "a) The backpropagation through time algorithm looks back at a window of 4 previous time steps. Draw this computation as an unfolded graph like Figure 10.3 in the [DL book](http://www.deeplearningbook.org/contents/rnn.html). (**2 points**)\n",
    "\n",
    "b) Which of the weight matrices used in the graph are same? Mark these arrows with the same symbol $W$. (**1 point**)\n",
    "\n",
    "c) This unfolded computational graph for Vanilla RNN can be represented by an equivalent Recursive Neural Network. Draw the architecture for this graph. (**1 point**)\n",
    "\n",
    "d) Can you construct a smaller height Recursive NN than c) with the same coverage of previous time steps? If no, then explain, else if yes, then draw this architecture? (**2 points**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Forget Gate in LSTMs (3 points)\n",
    "\n",
    "LSTMs forget information from its global cell state ($C_t$) that is irrelevant for prediction at the present time step by using the forget gate $f_t$: \n",
    "\n",
    "$C_t = f_t * C_{t-1}+i_t*\\tilde{C_t}$. \n",
    "\n",
    "Refer [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for more information on the notation. \n",
    "\n",
    "As this information ($f_t * C_{t-1}$) is forgotten, it might happen at some time in the future that the prediction process requires this information again, however, as this information is forgotten there is no way to access it again. Suggest a way of saving this information from being forgotten completely? Your solution should work systematically as the LSTM moves over a sequence. \n",
    "\n",
    "Hint: Do you know about Caching Mechanism in physical memories in computers?\n",
    "\n",
    "Ans: Need to save every cell state together with the hidden state that is generated in a cache. During the generation of a cell state look for similar cached hidden states than the current one and draw informations from it for the new cell state generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. Recurrent and Recursive Neural Networks: Theory (3 points)\n",
    "\n",
    "Following are statements that you should answer with either a True or a False. And, also provide a justification as to why you think so. To answer these questions, you will need to revisit the lecture slides and read the DL book's Chapter on [Sequence Modelling](http://www.deeplearningbook.org/contents/rnn.html).\n",
    "\n",
    "a) A Convolution Neural Network layer forms a shallower way of sharing parameters through time than a Vanilla RNN layer. \\[T/F\\]\n",
    "\n",
    "Ans: True. \n",
    "\n",
    "A convolutional network can be used to share paramters over time but it is shallow. And the output of the recurrent networks depend on the output of previous steps.\n",
    "\n",
    "b) Networks with output recurrence are more powerful than hidden-to-hidden recurrence. \\[T/F\\]\n",
    "\n",
    "Ans: False. \n",
    "\n",
    "Unless the output is very high-dimensional and rich, it will usually lack important information from the past. It might be easier to train though because each steps can be trained on its own and therefore be parallalized.\n",
    "\n",
    "c) Removing the Global Cell State from LSTMs will result in a Vanilla RNN. \\[T/F\\]\n",
    "\n",
    "Ans: True. \n",
    "\n",
    "Its clear that all gates depend on the cell state,so if the cell state vanishes the Cell will be a vanilla RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as the solution. The naming should include the assignment number and matriculation IDs of all members in your team in the following format:\n",
    "**assignment-11_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 members in a team). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please submit the solution to your tutor (with **[NNIA][assignment-11]** in email subject):\n",
    "1. Maksym Andriushchenko <s8mmandr@stud.uni-saarland.de>\n",
    "2. Marius Mosbach <s9msmosb@stud.uni-saarland.de>\n",
    "3. Rajarshi Biswas <rbisw17@gmail.com>\n",
    "4. Marimuthu Kalimuthu <s8makali@stud.uni-saarland.de>\n",
    "\n",
    "Note :  **If you are in a team, you should submit only 1 solution to only 1 tutor.** <br>\n",
    "$\\hspace{2em}$ **Submissions violating these rules will not be graded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 6:  Regularization methods in Machine learning and their application in Feedforward neural networks  (deadline: 16 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Regularization methods in ML $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **L2** and **L1** regularization on the weights used for modelling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** is very similar to least squares, except that the weights are estimated by minimizing a slightly different quantity. In particular, the ridge regression co-efficient estimates $\\mathbf{W_{ridge}}$ are the values that minimize, \n",
    "\n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{2}^2}$$ \n",
    "\n",
    "where,\n",
    "\n",
    "$\\mathbf{\\lambda>0}$ is the regularizer,\n",
    "\n",
    "**X** is the design matrix,\n",
    "\n",
    "$\\mathbf{W}$ is the weight vector and\n",
    "\n",
    "**Y** represents the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** seeks weight estimate $W^{Ridge}$ that fit the data well by minimizing the squared error $~$ $\\mathbf{||~Y-XW~||^2}$ (which was also the linear regression cost function).\n",
    "However, the second term, $\\mathbf{||~W~||^2}$, called a ***shrinkage penalty*** is small when $\\mathbf{W}$, i.e., $(w_1, w_2, ..., w_d)^T$ are close to zero. Thus, it has the effect of shrinking the estimates of $w_i$ towards zero.\n",
    "\n",
    "The regularizer $\\mathbf{\\lambda}$ serves to control the relative impact of these two terms on the regression weight estimates. when $\\mathbf{\\lambda=0}$, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\\mathbf{\\lambda \\rightarrow \\infty}$, the impact of the shrinkage penalty grows and the ridge regression weight estimates will approach zero. Unlike least squares, which generates only one set of weight estimates, ridge regression will produce a different set of weight estimates, $\\mathbf{W_{\\lambda}^{Ridge}}$, for each value of $\\mathbf{\\lambda}$. Hence, selecting a good value of $\\mathbf{\\lambda}$ is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ **Plot the magnitude of each weight in $\\mathbf{W^{Ridge}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Ridge weights $\\mathbf{W^{Ridge}}$.** $~$ ($2.5$ points)\n",
    "\n",
    "Download the dataset, **data.csv**, from the NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "# TODO Implement\n",
    "\n",
    "# Read 'Salary' as your response/dependent variable\n",
    "# TODO Implement\n",
    "\n",
    "# Drop the column with the dependent variable 'Salary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize values for the alphas\n",
    "lamdas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# Create a Ridge Object that performs ridge regression\n",
    "# TODO Implement\n",
    "\n",
    "# Create list to hold ridge weights\n",
    "# TODO Implement\n",
    "\n",
    "# Iterate over all lamdas, performing data fitting with ridge regression \n",
    "# and find the corresponding co-efficients\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "\n",
    "#ax.set_xscale('log')\n",
    "#plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Next we deal with **L1 regularization** for which the corresponding method is called **Lasso.** In Lasso, we minimize the function, \n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the magnitude of each weight in $\\mathbf{W^{Lasso}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Lasso weights $\\mathbf{W^{Lasso}}$.** $~$ ($2.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Lasso Object(set max_iter to 10000)\n",
    "# TODO Implement\n",
    "\n",
    "# Create list to hold lasso weights\n",
    "# TODO Implement\n",
    "\n",
    "# Iterate over all alphas, performing data fitting with Lasso\n",
    "# and find the corresponding co-efficients\n",
    "# TODO Implement\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "\n",
    "#ax.set_xscale('log')\n",
    "#plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the plots generated in problems $1$ and $2$ respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Impact of norms in the  Regularizer $~$ (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Assume$~$ $\\mathbf{x} \\in R^2$, $(x_1, x_2) \\in [-1, 1]\\times[-1, 1]$. $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, draw the contour plots for $\\mathbf{\\big|\\big|~x~\\big|\\big|_{0}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{1}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{2}}$ and $\\mathbf{\\big|\\big|~x~\\big|\\big|_{\\infty}}$ norms (consider all possible isolines in the given interval,i.e., ($[-1,1]\\times[-1,1]$) and **explain** how you get the corresponding plot, i.e., provide the mathematical formula for getting the outermost isoline in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Sketch the **Lasso** optimization function, $~$ $\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$ $~$ in two dimensions. From this sketch try to explain why **Lasso** induces **sparsity.** $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know Back-Propagation in details $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://github.com/mmarius/nnia-tutorial/blob/master/neural-net.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a **Feedforward Neural network** with one **input layer**, one **hidden layer** and one **output layer.** The **hidden layer** and **output layer** use the sigmoid function, $\\mathbf{\\sigma(x) = \\frac{1}{1+exp(-x)}}$, as **activation function.** Also note, that the network minimizes **Binary Cross Entropy loss**, given by, $$\\mathbf{J = \\sum -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})}$$\n",
    "\n",
    "We consider the true class labels to be **binary**, i.e., either $0$ or $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the purpose of computing the derivatives of the loss/cost function consider the numerical values obtained by the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer** consists of two nodes, $x_1$ and $x_2$ respectively. For our problem consider the following input,\n",
    "$$\\begin{bmatrix} x_1\\\\ x_2\\\\ \\end{bmatrix} = \\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden layer** is made up of 3 neurons and the corresponding matrix of weights is as given:\n",
    "$$\n",
    "\\mathbf{W_{hidden}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1^{1} & w_1^{2} & w_1^{3} \\\\ w_2^{1} & w_2^{2} & w_2^{3} \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.15 & -0.25 & 0.05\\\\ 0.20 & 0.10 & -0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Output of **Hidden layer** is given by, $~~$ $\\mathbf{a=\\sigma~(W_{hidden}^{T}x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer** consists of one neuron, i.e., the **network** generates a single output. **For our problem, the true class label is $1$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix corresponding to the **Output layer** is given by,\n",
    "$$\n",
    "\\mathbf{W_{out}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1\\\\w_2 \\\\w_3\\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.20\\\\-0.35\\\\0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** output from the **Output layer** is given by, $~$ $\\mathbf{\\hat{y} = \\sigma~(W_{out}^Ta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$6$. Execute the following sequence of operations and **show that Binary Cross Entropy loss is getting reduced, i.e., $ C^2 < C^1$** $~$ ($3$ points)\n",
    "\n",
    "**Perform Forward-propagation to generate output** $\\to$ **Compute loss or cost ($C^1$)** $\\to$ **perform Back-propagation to compute the error** $\\to$ **perform Gradient descent to update the weights** $\\to$ **peform Forward-propagation again with updated weights** $\\to$ **Compute loss or cost ($C^2$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  $C^i$ denotes the loss or cost at the $i^{th}$ iteration, for performing Gradient descent consider a learning rate of $0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_1 is :  [[ 0.71094875]]\n",
      "cost_2 is :  [[ 0.70631927]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return expit(x)\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def sigmoid_prime(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1.0 - sig)\n",
    "\n",
    "\n",
    "#Input array\n",
    "x=np.array([[-1],[1]])\n",
    "\n",
    "#Output\n",
    "y=1\n",
    "\n",
    "# weights\n",
    "w_hidden = np.array([[0.15, -0.25, 0.05],[0.20, 0.10, -0.15]])\n",
    "w_out = np.array([[0.20],[-0.35],[0.15]])\n",
    "lr = 0.1\n",
    "\n",
    "## forward propagation\n",
    "z1 = w_hidden.T.dot(x)\n",
    "a = sigmoid(z1)\n",
    "z2 = w_out.T.dot(a)\n",
    "y_pred = sigmoid(z2)\n",
    "\n",
    "cost_1 = -y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred)\n",
    "\n",
    "#Backpropagation\n",
    "E = y-y_pred\n",
    "slope_output_layer = sigmoid_prime(y_pred)\n",
    "slope_hidden_layer = sigmoid_prime(a)\n",
    "d_output = E * slope_output_layer\n",
    "Error_at_hidden_layer = d_output.dot(w_out.T)\n",
    "d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "\n",
    "## gradient descent\n",
    "w_out += y_pred.T.dot(d_output) *lr\n",
    "w_hidden += z1.T.dot(d_hiddenlayer) *lr\n",
    "\n",
    "## forward propagation\n",
    "z1_2 = w_hidden.T.dot(x)\n",
    "a_2 = sigmoid(z1_2)\n",
    "z2_2 = w_out.T.dot(a_2)\n",
    "y_pred_2 = sigmoid(z2_2)\n",
    "\n",
    "cost_2 = -y * np.log(y_pred_2) - (1 - y) * np.log(1 - y_pred_2)\n",
    "\n",
    "#print (\"z1\", z1)\n",
    "#print (\"a\", a)\n",
    "#print (\"z2\", z2)\n",
    "#print (\"pred is : \", y_pred)\n",
    "print (\"cost_1 is : \", cost_1)\n",
    "print (\"cost_2 is : \", cost_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward Neural Network with L1 and L2 regularization $~$ (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using only **Numpy** in python. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L1 and L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **mnist** dataset from NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"function for loading data\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network with a single hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels. \n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float\n",
    "        Regularizer for L1-regularization.\n",
    "        l1=0.0 implies no regularization\n",
    "        \n",
    "    l2 : float\n",
    "        Lambda value for L2-regularization.\n",
    "        l2=0.0 implies no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "        \n",
    "        #TODO Implement\n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output # np.unique(labels)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.w1, self.w2 = self.initialize_weights()\n",
    "\n",
    "\n",
    "    def encode_labels(self, y, k):\n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for i, val in enumerate(y):\n",
    "            onehot[val, i] = 1.0\n",
    "        return onehot\n",
    "        \n",
    "        return one_hot_targets\n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize using random numbers.\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\n",
    "           Implement a stable version which \n",
    "           takes care of overflow and underflow.\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        return expit(z)\n",
    "        \n",
    "\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        sig = self.sigmoid(z)\n",
    "        return sig * (1.0 - sig)\n",
    "        \n",
    "\n",
    "    def add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit to array at index 0\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        \n",
    "        return X_new\n",
    "        \n",
    "        \n",
    "\n",
    "    def feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array,\n",
    "            Input values with bias unit.\n",
    "        z2 : array,\n",
    "            Net input of hidden layer.\n",
    "        a2 : array,\n",
    "            Activation of hidden layer.\n",
    "        z3 : array,\n",
    "            Net input of output layer.\n",
    "        a3 : array,\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        a1 = self.add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        a2 = self.add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self.sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "        #linear_model = self.sigmoid(np.dot(w1, X))\n",
    "        #output = self.sigmoid(np.dot(linear_model, w2))\n",
    "        #return output\n",
    "        \n",
    "\n",
    "    def L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        l2_reg = (lambda_ /2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:] ** 2))\n",
    "        return  l2_reg \n",
    "        \n",
    "        \n",
    "\n",
    "    def L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        l1_reg = (lambda_ /2.0) * (np.abs(w1[:, 1:]).sum() + np.abs(w2[:, 1:]).sum())\n",
    "        return l1_reg\n",
    "        \n",
    "    def get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        \n",
    "        output : array, Activation of the output layer (feedforward)\n",
    "        \n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        l1_reg = self.L1_reg(self.l1, w1, w2)\n",
    "        l2_reg = self.L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + l1_reg + l2_reg\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, Input values with bias unit.\n",
    "        a2 : array, Activation of hidden layer.\n",
    "        a3 : array, Activation of output layer.\n",
    "        z2 : array, Net input of hidden layer.\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        #TODO Implement\n",
    "        output_layer = a3 - y_enc\n",
    "        z2 = self.add_bias_unit(z2, how='row')\n",
    "        hidden_layer = w2.T.dot(output_layer) * self.sigmoid_gradient(z2)\n",
    "        hidden_layer = hidden_layer[1:, :]\n",
    "        grad1 = hidden_layer.dot(a1)\n",
    "        grad2 = output_layer.dot(a2.T)\n",
    "        \n",
    "\n",
    "        # regularize\n",
    "        #TODO Implement\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        \n",
    "        a1, z2, a2, z3, a3 = self.feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "        y : array, Target class labels.\n",
    "        print_progress : bool, Prints the progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #w1=self.w1\n",
    "        #w2=self.w2\n",
    "        #print (w1, w2)\n",
    "        #TODO Implement\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self.encode_labels(y, self.n_output)\n",
    "\n",
    "        w1_prev = np.zeros(self.w1.shape)\n",
    "        w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self.feedforward(X_data[idx], self.w1, self.w2)\n",
    "                cost = self.get_cost(y_enc=y_enc[:, idx], output=a3, self.w1, self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # backpropagation\n",
    "                grad1, grad2 = self.get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                d_w1, d_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (d_w1 + (self.eta * w1_prev))\n",
    "                self.w2 -= (d_w2 + (self.eta * w2_prev))\n",
    "                w1_prev, w2_prev = d_w1, d_w2\n",
    "\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = MLP(n_output=10, \n",
    "                  n_features=X_train.shape[1], \n",
    "                  n_hidden=50, \n",
    "                  l2=0.1, \n",
    "                  l1=0.0, \n",
    "                  epochs=1000, \n",
    "                  eta=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  minibatches=50, \n",
    "                  shuffle=True,\n",
    "                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000\n",
      "Epoch: 2/1000\n",
      "Epoch: 3/1000\n",
      "Epoch: 4/1000\n",
      "Epoch: 5/1000\n",
      "Epoch: 6/1000\n",
      "Epoch: 7/1000\n",
      "Epoch: 8/1000\n",
      "Epoch: 9/1000\n",
      "Epoch: 10/1000\n",
      "Epoch: 11/1000\n",
      "Epoch: 12/1000\n",
      "Epoch: 13/1000\n",
      "Epoch: 14/1000\n",
      "Epoch: 15/1000\n",
      "Epoch: 16/1000\n",
      "Epoch: 17/1000\n",
      "Epoch: 18/1000\n",
      "Epoch: 19/1000\n",
      "Epoch: 20/1000\n",
      "Epoch: 21/1000\n",
      "Epoch: 22/1000\n",
      "Epoch: 23/1000\n",
      "Epoch: 24/1000\n",
      "Epoch: 25/1000\n",
      "Epoch: 26/1000\n",
      "Epoch: 27/1000\n",
      "Epoch: 28/1000\n",
      "Epoch: 29/1000\n",
      "Epoch: 30/1000\n",
      "Epoch: 31/1000\n",
      "Epoch: 32/1000\n",
      "Epoch: 33/1000\n",
      "Epoch: 34/1000\n",
      "Epoch: 35/1000\n",
      "Epoch: 36/1000\n",
      "Epoch: 37/1000\n",
      "Epoch: 38/1000\n",
      "Epoch: 39/1000\n",
      "Epoch: 40/1000\n",
      "Epoch: 41/1000\n",
      "Epoch: 42/1000\n",
      "Epoch: 43/1000\n",
      "Epoch: 44/1000\n",
      "Epoch: 45/1000\n",
      "Epoch: 46/1000\n",
      "Epoch: 47/1000\n",
      "Epoch: 48/1000\n",
      "Epoch: 49/1000\n",
      "Epoch: 50/1000\n",
      "Epoch: 51/1000\n",
      "Epoch: 52/1000\n",
      "Epoch: 53/1000\n",
      "Epoch: 54/1000\n",
      "Epoch: 55/1000\n",
      "Epoch: 56/1000\n",
      "Epoch: 57/1000\n",
      "Epoch: 58/1000\n",
      "Epoch: 59/1000\n",
      "Epoch: 60/1000\n",
      "Epoch: 61/1000\n",
      "Epoch: 62/1000\n",
      "Epoch: 63/1000\n",
      "Epoch: 64/1000\n",
      "Epoch: 65/1000\n",
      "Epoch: 66/1000\n",
      "Epoch: 67/1000\n",
      "Epoch: 68/1000\n",
      "Epoch: 69/1000\n",
      "Epoch: 70/1000\n",
      "Epoch: 71/1000\n",
      "Epoch: 72/1000\n",
      "Epoch: 73/1000\n",
      "Epoch: 74/1000\n",
      "Epoch: 75/1000\n",
      "Epoch: 76/1000\n",
      "Epoch: 77/1000\n",
      "Epoch: 78/1000\n",
      "Epoch: 79/1000\n",
      "Epoch: 80/1000\n",
      "Epoch: 81/1000\n",
      "Epoch: 82/1000\n",
      "Epoch: 83/1000\n",
      "Epoch: 84/1000\n",
      "Epoch: 85/1000\n",
      "Epoch: 86/1000\n",
      "Epoch: 87/1000\n",
      "Epoch: 88/1000\n",
      "Epoch: 89/1000\n",
      "Epoch: 90/1000\n",
      "Epoch: 91/1000\n",
      "Epoch: 92/1000\n",
      "Epoch: 93/1000\n",
      "Epoch: 94/1000\n",
      "Epoch: 95/1000\n",
      "Epoch: 96/1000\n",
      "Epoch: 97/1000\n",
      "Epoch: 98/1000\n",
      "Epoch: 99/1000\n",
      "Epoch: 100/1000\n",
      "Epoch: 101/1000\n",
      "Epoch: 102/1000\n",
      "Epoch: 103/1000\n",
      "Epoch: 104/1000\n",
      "Epoch: 105/1000\n",
      "Epoch: 106/1000\n",
      "Epoch: 107/1000\n",
      "Epoch: 108/1000\n",
      "Epoch: 109/1000\n",
      "Epoch: 110/1000\n",
      "Epoch: 111/1000\n",
      "Epoch: 112/1000\n",
      "Epoch: 113/1000\n",
      "Epoch: 114/1000\n",
      "Epoch: 115/1000\n",
      "Epoch: 116/1000\n",
      "Epoch: 117/1000\n",
      "Epoch: 118/1000\n",
      "Epoch: 119/1000\n",
      "Epoch: 120/1000\n",
      "Epoch: 121/1000\n",
      "Epoch: 122/1000\n",
      "Epoch: 123/1000\n",
      "Epoch: 124/1000\n",
      "Epoch: 125/1000\n",
      "Epoch: 126/1000\n",
      "Epoch: 127/1000\n",
      "Epoch: 128/1000\n",
      "Epoch: 129/1000\n",
      "Epoch: 130/1000\n",
      "Epoch: 131/1000\n",
      "Epoch: 132/1000\n",
      "Epoch: 133/1000\n",
      "Epoch: 134/1000\n",
      "Epoch: 135/1000\n",
      "Epoch: 136/1000\n",
      "Epoch: 137/1000\n",
      "Epoch: 138/1000\n",
      "Epoch: 139/1000\n",
      "Epoch: 140/1000\n",
      "Epoch: 141/1000\n",
      "Epoch: 142/1000\n",
      "Epoch: 143/1000\n",
      "Epoch: 144/1000\n",
      "Epoch: 145/1000\n",
      "Epoch: 146/1000\n",
      "Epoch: 147/1000\n",
      "Epoch: 148/1000\n",
      "Epoch: 149/1000\n",
      "Epoch: 150/1000\n",
      "Epoch: 151/1000\n",
      "Epoch: 152/1000\n",
      "Epoch: 153/1000\n",
      "Epoch: 154/1000\n",
      "Epoch: 155/1000\n",
      "Epoch: 156/1000\n",
      "Epoch: 157/1000\n",
      "Epoch: 158/1000\n",
      "Epoch: 159/1000\n",
      "Epoch: 160/1000\n",
      "Epoch: 161/1000\n",
      "Epoch: 162/1000\n",
      "Epoch: 163/1000\n",
      "Epoch: 164/1000\n",
      "Epoch: 165/1000\n",
      "Epoch: 166/1000\n",
      "Epoch: 167/1000\n",
      "Epoch: 168/1000\n",
      "Epoch: 169/1000\n",
      "Epoch: 170/1000\n",
      "Epoch: 171/1000\n",
      "Epoch: 172/1000\n",
      "Epoch: 173/1000\n",
      "Epoch: 174/1000\n",
      "Epoch: 175/1000\n",
      "Epoch: 176/1000\n",
      "Epoch: 177/1000\n",
      "Epoch: 178/1000\n",
      "Epoch: 179/1000\n",
      "Epoch: 180/1000\n",
      "Epoch: 181/1000\n",
      "Epoch: 182/1000\n",
      "Epoch: 183/1000\n",
      "Epoch: 184/1000\n",
      "Epoch: 185/1000\n",
      "Epoch: 186/1000\n",
      "Epoch: 187/1000\n",
      "Epoch: 188/1000\n",
      "Epoch: 189/1000\n",
      "Epoch: 190/1000\n",
      "Epoch: 191/1000\n",
      "Epoch: 192/1000\n",
      "Epoch: 193/1000\n",
      "Epoch: 194/1000\n",
      "Epoch: 195/1000\n",
      "Epoch: 196/1000\n",
      "Epoch: 197/1000\n",
      "Epoch: 198/1000\n",
      "Epoch: 199/1000\n",
      "Epoch: 200/1000\n",
      "Epoch: 201/1000\n",
      "Epoch: 202/1000\n",
      "Epoch: 203/1000\n",
      "Epoch: 204/1000\n",
      "Epoch: 205/1000\n",
      "Epoch: 206/1000\n",
      "Epoch: 207/1000\n",
      "Epoch: 208/1000\n",
      "Epoch: 209/1000\n",
      "Epoch: 210/1000\n",
      "Epoch: 211/1000\n",
      "Epoch: 212/1000\n",
      "Epoch: 213/1000\n",
      "Epoch: 214/1000\n",
      "Epoch: 215/1000\n",
      "Epoch: 216/1000\n",
      "Epoch: 217/1000\n",
      "Epoch: 218/1000\n",
      "Epoch: 219/1000\n",
      "Epoch: 220/1000\n",
      "Epoch: 221/1000\n",
      "Epoch: 222/1000\n",
      "Epoch: 223/1000\n",
      "Epoch: 224/1000\n",
      "Epoch: 225/1000\n",
      "Epoch: 226/1000\n",
      "Epoch: 227/1000\n",
      "Epoch: 228/1000\n",
      "Epoch: 229/1000\n",
      "Epoch: 230/1000\n",
      "Epoch: 231/1000\n",
      "Epoch: 232/1000\n",
      "Epoch: 233/1000\n",
      "Epoch: 234/1000\n",
      "Epoch: 235/1000\n",
      "Epoch: 236/1000\n",
      "Epoch: 237/1000\n",
      "Epoch: 238/1000\n",
      "Epoch: 239/1000\n",
      "Epoch: 240/1000\n",
      "Epoch: 241/1000\n",
      "Epoch: 242/1000\n",
      "Epoch: 243/1000\n",
      "Epoch: 244/1000\n",
      "Epoch: 245/1000\n",
      "Epoch: 246/1000\n",
      "Epoch: 247/1000\n",
      "Epoch: 248/1000\n",
      "Epoch: 249/1000\n",
      "Epoch: 250/1000\n",
      "Epoch: 251/1000\n",
      "Epoch: 252/1000\n",
      "Epoch: 253/1000\n",
      "Epoch: 254/1000\n",
      "Epoch: 255/1000\n",
      "Epoch: 256/1000\n",
      "Epoch: 257/1000\n",
      "Epoch: 258/1000\n",
      "Epoch: 259/1000\n",
      "Epoch: 260/1000\n",
      "Epoch: 261/1000\n",
      "Epoch: 262/1000\n",
      "Epoch: 263/1000\n",
      "Epoch: 264/1000\n",
      "Epoch: 265/1000\n",
      "Epoch: 266/1000\n",
      "Epoch: 267/1000\n",
      "Epoch: 268/1000\n",
      "Epoch: 269/1000\n",
      "Epoch: 270/1000\n",
      "Epoch: 271/1000\n",
      "Epoch: 272/1000\n",
      "Epoch: 273/1000\n",
      "Epoch: 274/1000\n",
      "Epoch: 275/1000\n",
      "Epoch: 276/1000\n",
      "Epoch: 277/1000\n",
      "Epoch: 278/1000\n",
      "Epoch: 279/1000\n",
      "Epoch: 280/1000\n",
      "Epoch: 281/1000\n",
      "Epoch: 282/1000\n",
      "Epoch: 283/1000\n",
      "Epoch: 284/1000\n",
      "Epoch: 285/1000\n",
      "Epoch: 286/1000\n",
      "Epoch: 287/1000\n",
      "Epoch: 288/1000\n",
      "Epoch: 289/1000\n",
      "Epoch: 290/1000\n",
      "Epoch: 291/1000\n",
      "Epoch: 292/1000\n",
      "Epoch: 293/1000\n",
      "Epoch: 294/1000\n",
      "Epoch: 295/1000\n",
      "Epoch: 296/1000\n",
      "Epoch: 297/1000\n",
      "Epoch: 298/1000\n",
      "Epoch: 299/1000\n",
      "Epoch: 300/1000\n",
      "Epoch: 301/1000\n",
      "Epoch: 302/1000\n",
      "Epoch: 303/1000\n",
      "Epoch: 304/1000\n",
      "Epoch: 305/1000\n",
      "Epoch: 306/1000\n",
      "Epoch: 307/1000\n",
      "Epoch: 308/1000\n",
      "Epoch: 309/1000\n",
      "Epoch: 310/1000\n",
      "Epoch: 311/1000\n",
      "Epoch: 312/1000\n",
      "Epoch: 313/1000\n",
      "Epoch: 314/1000\n",
      "Epoch: 315/1000\n",
      "Epoch: 316/1000\n",
      "Epoch: 317/1000\n",
      "Epoch: 318/1000\n",
      "Epoch: 319/1000\n",
      "Epoch: 320/1000\n",
      "Epoch: 321/1000\n",
      "Epoch: 322/1000\n",
      "Epoch: 323/1000\n",
      "Epoch: 324/1000\n",
      "Epoch: 325/1000\n",
      "Epoch: 326/1000\n",
      "Epoch: 327/1000\n",
      "Epoch: 328/1000\n",
      "Epoch: 329/1000\n",
      "Epoch: 330/1000\n",
      "Epoch: 331/1000\n",
      "Epoch: 332/1000\n",
      "Epoch: 333/1000\n",
      "Epoch: 334/1000\n",
      "Epoch: 335/1000\n",
      "Epoch: 336/1000\n",
      "Epoch: 337/1000\n",
      "Epoch: 338/1000\n",
      "Epoch: 339/1000\n",
      "Epoch: 340/1000\n",
      "Epoch: 341/1000\n",
      "Epoch: 342/1000\n",
      "Epoch: 343/1000\n",
      "Epoch: 344/1000\n",
      "Epoch: 345/1000\n",
      "Epoch: 346/1000\n",
      "Epoch: 347/1000\n",
      "Epoch: 348/1000\n",
      "Epoch: 349/1000\n",
      "Epoch: 350/1000\n",
      "Epoch: 351/1000\n",
      "Epoch: 352/1000\n",
      "Epoch: 353/1000\n",
      "Epoch: 354/1000\n",
      "Epoch: 355/1000\n",
      "Epoch: 356/1000\n",
      "Epoch: 357/1000\n",
      "Epoch: 358/1000\n",
      "Epoch: 359/1000\n",
      "Epoch: 360/1000\n",
      "Epoch: 361/1000\n",
      "Epoch: 362/1000\n",
      "Epoch: 363/1000\n",
      "Epoch: 364/1000\n",
      "Epoch: 365/1000\n",
      "Epoch: 366/1000\n",
      "Epoch: 367/1000\n",
      "Epoch: 368/1000\n",
      "Epoch: 369/1000\n",
      "Epoch: 370/1000\n",
      "Epoch: 371/1000\n",
      "Epoch: 372/1000\n",
      "Epoch: 373/1000\n",
      "Epoch: 374/1000\n",
      "Epoch: 375/1000\n",
      "Epoch: 376/1000\n",
      "Epoch: 377/1000\n",
      "Epoch: 378/1000\n",
      "Epoch: 379/1000\n",
      "Epoch: 380/1000\n",
      "Epoch: 381/1000\n",
      "Epoch: 382/1000\n",
      "Epoch: 383/1000\n",
      "Epoch: 384/1000\n",
      "Epoch: 385/1000\n",
      "Epoch: 386/1000\n",
      "Epoch: 387/1000\n",
      "Epoch: 388/1000\n",
      "Epoch: 389/1000\n",
      "Epoch: 390/1000\n",
      "Epoch: 391/1000\n",
      "Epoch: 392/1000\n",
      "Epoch: 393/1000\n",
      "Epoch: 394/1000\n",
      "Epoch: 395/1000\n",
      "Epoch: 396/1000\n",
      "Epoch: 397/1000\n",
      "Epoch: 398/1000\n",
      "Epoch: 399/1000\n",
      "Epoch: 400/1000\n",
      "Epoch: 401/1000\n",
      "Epoch: 402/1000\n",
      "Epoch: 403/1000\n",
      "Epoch: 404/1000\n",
      "Epoch: 405/1000\n",
      "Epoch: 406/1000\n",
      "Epoch: 407/1000\n",
      "Epoch: 408/1000\n",
      "Epoch: 409/1000\n",
      "Epoch: 410/1000\n",
      "Epoch: 411/1000\n",
      "Epoch: 412/1000\n",
      "Epoch: 413/1000\n",
      "Epoch: 414/1000\n",
      "Epoch: 415/1000\n",
      "Epoch: 416/1000\n",
      "Epoch: 417/1000\n",
      "Epoch: 418/1000\n",
      "Epoch: 419/1000\n",
      "Epoch: 420/1000\n",
      "Epoch: 421/1000\n",
      "Epoch: 422/1000\n",
      "Epoch: 423/1000\n",
      "Epoch: 424/1000\n",
      "Epoch: 425/1000\n",
      "Epoch: 426/1000\n",
      "Epoch: 427/1000\n",
      "Epoch: 428/1000\n",
      "Epoch: 429/1000\n",
      "Epoch: 430/1000\n",
      "Epoch: 431/1000\n",
      "Epoch: 432/1000\n",
      "Epoch: 433/1000\n",
      "Epoch: 434/1000\n",
      "Epoch: 435/1000\n",
      "Epoch: 436/1000\n",
      "Epoch: 437/1000\n",
      "Epoch: 438/1000\n",
      "Epoch: 439/1000\n",
      "Epoch: 440/1000\n",
      "Epoch: 441/1000\n",
      "Epoch: 442/1000\n",
      "Epoch: 443/1000\n",
      "Epoch: 444/1000\n",
      "Epoch: 445/1000\n",
      "Epoch: 446/1000\n",
      "Epoch: 447/1000\n",
      "Epoch: 448/1000\n",
      "Epoch: 449/1000\n",
      "Epoch: 450/1000\n",
      "Epoch: 451/1000\n",
      "Epoch: 452/1000\n",
      "Epoch: 453/1000\n",
      "Epoch: 454/1000\n",
      "Epoch: 455/1000\n",
      "Epoch: 456/1000\n",
      "Epoch: 457/1000\n",
      "Epoch: 458/1000\n",
      "Epoch: 459/1000\n",
      "Epoch: 460/1000\n",
      "Epoch: 461/1000\n",
      "Epoch: 462/1000\n",
      "Epoch: 463/1000\n",
      "Epoch: 464/1000\n",
      "Epoch: 465/1000\n",
      "Epoch: 466/1000\n",
      "Epoch: 467/1000\n",
      "Epoch: 468/1000\n",
      "Epoch: 469/1000\n",
      "Epoch: 470/1000\n",
      "Epoch: 471/1000\n",
      "Epoch: 472/1000\n",
      "Epoch: 473/1000\n",
      "Epoch: 474/1000\n",
      "Epoch: 475/1000\n",
      "Epoch: 476/1000\n",
      "Epoch: 477/1000\n",
      "Epoch: 478/1000\n",
      "Epoch: 479/1000\n",
      "Epoch: 480/1000\n",
      "Epoch: 481/1000\n",
      "Epoch: 482/1000\n",
      "Epoch: 483/1000\n",
      "Epoch: 484/1000\n",
      "Epoch: 485/1000\n",
      "Epoch: 486/1000\n",
      "Epoch: 487/1000\n",
      "Epoch: 488/1000\n",
      "Epoch: 489/1000\n",
      "Epoch: 490/1000\n",
      "Epoch: 491/1000\n",
      "Epoch: 492/1000\n",
      "Epoch: 493/1000\n",
      "Epoch: 494/1000\n",
      "Epoch: 495/1000\n",
      "Epoch: 496/1000\n",
      "Epoch: 497/1000\n",
      "Epoch: 498/1000\n",
      "Epoch: 499/1000\n",
      "Epoch: 500/1000\n",
      "Epoch: 501/1000\n",
      "Epoch: 502/1000\n",
      "Epoch: 503/1000\n",
      "Epoch: 504/1000\n",
      "Epoch: 505/1000\n",
      "Epoch: 506/1000\n",
      "Epoch: 507/1000\n",
      "Epoch: 508/1000\n",
      "Epoch: 509/1000\n",
      "Epoch: 510/1000\n",
      "Epoch: 511/1000\n",
      "Epoch: 512/1000\n",
      "Epoch: 513/1000\n",
      "Epoch: 514/1000\n",
      "Epoch: 515/1000\n",
      "Epoch: 516/1000\n",
      "Epoch: 517/1000\n",
      "Epoch: 518/1000\n",
      "Epoch: 519/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 520/1000\n",
      "Epoch: 521/1000\n",
      "Epoch: 522/1000\n",
      "Epoch: 523/1000\n",
      "Epoch: 524/1000\n",
      "Epoch: 525/1000\n",
      "Epoch: 526/1000\n",
      "Epoch: 527/1000\n",
      "Epoch: 528/1000\n",
      "Epoch: 529/1000\n",
      "Epoch: 530/1000\n",
      "Epoch: 531/1000\n",
      "Epoch: 532/1000\n",
      "Epoch: 533/1000\n",
      "Epoch: 534/1000\n",
      "Epoch: 535/1000\n",
      "Epoch: 536/1000\n",
      "Epoch: 537/1000\n",
      "Epoch: 538/1000\n",
      "Epoch: 539/1000\n",
      "Epoch: 540/1000\n",
      "Epoch: 541/1000\n",
      "Epoch: 542/1000\n",
      "Epoch: 543/1000\n",
      "Epoch: 544/1000\n",
      "Epoch: 545/1000\n",
      "Epoch: 546/1000\n",
      "Epoch: 547/1000\n",
      "Epoch: 548/1000\n",
      "Epoch: 549/1000\n",
      "Epoch: 550/1000\n",
      "Epoch: 551/1000\n",
      "Epoch: 552/1000\n",
      "Epoch: 553/1000\n",
      "Epoch: 554/1000\n",
      "Epoch: 555/1000\n",
      "Epoch: 556/1000\n",
      "Epoch: 557/1000\n",
      "Epoch: 558/1000\n",
      "Epoch: 559/1000\n",
      "Epoch: 560/1000\n",
      "Epoch: 561/1000\n",
      "Epoch: 562/1000\n",
      "Epoch: 563/1000\n",
      "Epoch: 564/1000\n",
      "Epoch: 565/1000\n",
      "Epoch: 566/1000\n",
      "Epoch: 567/1000\n",
      "Epoch: 568/1000\n",
      "Epoch: 569/1000\n",
      "Epoch: 570/1000\n",
      "Epoch: 571/1000\n",
      "Epoch: 572/1000\n",
      "Epoch: 573/1000\n",
      "Epoch: 574/1000\n",
      "Epoch: 575/1000\n",
      "Epoch: 576/1000\n",
      "Epoch: 577/1000\n",
      "Epoch: 578/1000\n",
      "Epoch: 579/1000\n",
      "Epoch: 580/1000\n",
      "Epoch: 581/1000\n",
      "Epoch: 582/1000\n",
      "Epoch: 583/1000\n",
      "Epoch: 584/1000\n",
      "Epoch: 585/1000\n",
      "Epoch: 586/1000\n",
      "Epoch: 587/1000\n",
      "Epoch: 588/1000\n",
      "Epoch: 589/1000\n",
      "Epoch: 590/1000\n",
      "Epoch: 591/1000\n",
      "Epoch: 592/1000\n",
      "Epoch: 593/1000\n",
      "Epoch: 594/1000\n",
      "Epoch: 595/1000\n",
      "Epoch: 596/1000\n",
      "Epoch: 597/1000\n",
      "Epoch: 598/1000\n",
      "Epoch: 599/1000\n",
      "Epoch: 600/1000\n",
      "Epoch: 601/1000\n",
      "Epoch: 602/1000\n",
      "Epoch: 603/1000\n",
      "Epoch: 604/1000\n",
      "Epoch: 605/1000\n",
      "Epoch: 606/1000\n",
      "Epoch: 607/1000\n",
      "Epoch: 608/1000\n",
      "Epoch: 609/1000\n",
      "Epoch: 610/1000\n",
      "Epoch: 611/1000\n",
      "Epoch: 612/1000\n",
      "Epoch: 613/1000\n",
      "Epoch: 614/1000\n",
      "Epoch: 615/1000\n",
      "Epoch: 616/1000\n",
      "Epoch: 617/1000\n",
      "Epoch: 618/1000\n",
      "Epoch: 619/1000\n",
      "Epoch: 620/1000\n",
      "Epoch: 621/1000\n",
      "Epoch: 622/1000\n",
      "Epoch: 623/1000\n",
      "Epoch: 624/1000\n",
      "Epoch: 625/1000\n",
      "Epoch: 626/1000\n",
      "Epoch: 627/1000\n",
      "Epoch: 628/1000\n",
      "Epoch: 629/1000\n",
      "Epoch: 630/1000\n",
      "Epoch: 631/1000\n",
      "Epoch: 632/1000\n",
      "Epoch: 633/1000\n",
      "Epoch: 634/1000\n",
      "Epoch: 635/1000\n",
      "Epoch: 636/1000\n",
      "Epoch: 637/1000\n",
      "Epoch: 638/1000\n",
      "Epoch: 639/1000\n",
      "Epoch: 640/1000\n",
      "Epoch: 641/1000\n",
      "Epoch: 642/1000\n",
      "Epoch: 643/1000\n",
      "Epoch: 644/1000\n",
      "Epoch: 645/1000\n",
      "Epoch: 646/1000\n",
      "Epoch: 647/1000\n",
      "Epoch: 648/1000\n",
      "Epoch: 649/1000\n",
      "Epoch: 650/1000\n",
      "Epoch: 651/1000\n",
      "Epoch: 652/1000\n",
      "Epoch: 653/1000\n",
      "Epoch: 654/1000\n",
      "Epoch: 655/1000\n",
      "Epoch: 656/1000\n",
      "Epoch: 657/1000\n",
      "Epoch: 658/1000\n",
      "Epoch: 659/1000\n",
      "Epoch: 660/1000\n",
      "Epoch: 661/1000\n",
      "Epoch: 662/1000\n",
      "Epoch: 663/1000\n",
      "Epoch: 664/1000\n",
      "Epoch: 665/1000\n",
      "Epoch: 666/1000\n",
      "Epoch: 667/1000\n",
      "Epoch: 668/1000\n",
      "Epoch: 669/1000\n",
      "Epoch: 670/1000\n",
      "Epoch: 671/1000\n",
      "Epoch: 672/1000\n",
      "Epoch: 673/1000\n",
      "Epoch: 674/1000\n",
      "Epoch: 675/1000\n",
      "Epoch: 676/1000\n",
      "Epoch: 677/1000\n",
      "Epoch: 678/1000\n",
      "Epoch: 679/1000\n",
      "Epoch: 680/1000\n",
      "Epoch: 681/1000\n",
      "Epoch: 682/1000\n",
      "Epoch: 683/1000\n",
      "Epoch: 684/1000\n",
      "Epoch: 685/1000\n",
      "Epoch: 686/1000\n",
      "Epoch: 687/1000\n",
      "Epoch: 688/1000\n",
      "Epoch: 689/1000\n",
      "Epoch: 690/1000\n",
      "Epoch: 691/1000\n",
      "Epoch: 692/1000\n",
      "Epoch: 693/1000\n",
      "Epoch: 694/1000\n",
      "Epoch: 695/1000\n",
      "Epoch: 696/1000\n",
      "Epoch: 697/1000\n",
      "Epoch: 698/1000\n",
      "Epoch: 699/1000\n",
      "Epoch: 700/1000\n",
      "Epoch: 701/1000\n",
      "Epoch: 702/1000\n",
      "Epoch: 703/1000\n",
      "Epoch: 704/1000\n",
      "Epoch: 705/1000\n",
      "Epoch: 706/1000\n",
      "Epoch: 707/1000\n",
      "Epoch: 708/1000\n",
      "Epoch: 709/1000\n",
      "Epoch: 710/1000\n",
      "Epoch: 711/1000\n",
      "Epoch: 712/1000\n",
      "Epoch: 713/1000\n",
      "Epoch: 714/1000\n",
      "Epoch: 715/1000\n",
      "Epoch: 716/1000\n",
      "Epoch: 717/1000\n",
      "Epoch: 718/1000\n",
      "Epoch: 719/1000\n",
      "Epoch: 720/1000\n",
      "Epoch: 721/1000\n",
      "Epoch: 722/1000\n",
      "Epoch: 723/1000\n",
      "Epoch: 724/1000\n",
      "Epoch: 725/1000\n",
      "Epoch: 726/1000\n",
      "Epoch: 727/1000\n",
      "Epoch: 728/1000\n",
      "Epoch: 729/1000\n",
      "Epoch: 730/1000\n",
      "Epoch: 731/1000\n",
      "Epoch: 732/1000\n",
      "Epoch: 733/1000\n",
      "Epoch: 734/1000\n",
      "Epoch: 735/1000\n",
      "Epoch: 736/1000\n",
      "Epoch: 737/1000\n",
      "Epoch: 738/1000\n",
      "Epoch: 739/1000\n",
      "Epoch: 740/1000\n",
      "Epoch: 741/1000\n",
      "Epoch: 742/1000\n",
      "Epoch: 743/1000\n",
      "Epoch: 744/1000\n",
      "Epoch: 745/1000\n",
      "Epoch: 746/1000\n",
      "Epoch: 747/1000\n",
      "Epoch: 748/1000\n",
      "Epoch: 749/1000\n",
      "Epoch: 750/1000\n",
      "Epoch: 751/1000\n",
      "Epoch: 752/1000\n",
      "Epoch: 753/1000\n",
      "Epoch: 754/1000\n",
      "Epoch: 755/1000\n",
      "Epoch: 756/1000\n",
      "Epoch: 757/1000\n",
      "Epoch: 758/1000\n",
      "Epoch: 759/1000\n",
      "Epoch: 760/1000\n",
      "Epoch: 761/1000\n",
      "Epoch: 762/1000\n",
      "Epoch: 763/1000\n",
      "Epoch: 764/1000\n",
      "Epoch: 765/1000\n",
      "Epoch: 766/1000\n",
      "Epoch: 767/1000\n",
      "Epoch: 768/1000\n",
      "Epoch: 769/1000\n",
      "Epoch: 770/1000\n",
      "Epoch: 771/1000\n",
      "Epoch: 772/1000\n",
      "Epoch: 773/1000\n",
      "Epoch: 774/1000\n",
      "Epoch: 775/1000\n",
      "Epoch: 776/1000\n",
      "Epoch: 777/1000\n",
      "Epoch: 778/1000\n",
      "Epoch: 779/1000\n",
      "Epoch: 780/1000\n",
      "Epoch: 781/1000\n",
      "Epoch: 782/1000\n",
      "Epoch: 783/1000\n",
      "Epoch: 784/1000\n",
      "Epoch: 785/1000\n",
      "Epoch: 786/1000\n",
      "Epoch: 787/1000\n",
      "Epoch: 788/1000\n",
      "Epoch: 789/1000\n",
      "Epoch: 790/1000\n",
      "Epoch: 791/1000\n",
      "Epoch: 792/1000\n",
      "Epoch: 793/1000\n",
      "Epoch: 794/1000\n",
      "Epoch: 795/1000\n",
      "Epoch: 796/1000\n",
      "Epoch: 797/1000\n",
      "Epoch: 798/1000\n",
      "Epoch: 799/1000\n",
      "Epoch: 800/1000\n",
      "Epoch: 801/1000\n",
      "Epoch: 802/1000\n",
      "Epoch: 803/1000\n",
      "Epoch: 804/1000\n",
      "Epoch: 805/1000\n",
      "Epoch: 806/1000\n",
      "Epoch: 807/1000\n",
      "Epoch: 808/1000\n",
      "Epoch: 809/1000\n",
      "Epoch: 810/1000\n",
      "Epoch: 811/1000\n",
      "Epoch: 812/1000\n",
      "Epoch: 813/1000\n",
      "Epoch: 814/1000\n",
      "Epoch: 815/1000\n",
      "Epoch: 816/1000\n",
      "Epoch: 817/1000\n",
      "Epoch: 818/1000\n",
      "Epoch: 819/1000\n",
      "Epoch: 820/1000\n",
      "Epoch: 821/1000\n",
      "Epoch: 822/1000\n",
      "Epoch: 823/1000\n",
      "Epoch: 824/1000\n",
      "Epoch: 825/1000\n",
      "Epoch: 826/1000\n",
      "Epoch: 827/1000\n",
      "Epoch: 828/1000\n",
      "Epoch: 829/1000\n",
      "Epoch: 830/1000\n",
      "Epoch: 831/1000\n",
      "Epoch: 832/1000\n",
      "Epoch: 833/1000\n",
      "Epoch: 834/1000\n",
      "Epoch: 835/1000\n",
      "Epoch: 836/1000\n",
      "Epoch: 837/1000\n",
      "Epoch: 838/1000\n",
      "Epoch: 839/1000\n",
      "Epoch: 840/1000\n",
      "Epoch: 841/1000\n",
      "Epoch: 842/1000\n",
      "Epoch: 843/1000\n",
      "Epoch: 844/1000\n",
      "Epoch: 845/1000\n",
      "Epoch: 846/1000\n",
      "Epoch: 847/1000\n",
      "Epoch: 848/1000\n",
      "Epoch: 849/1000\n",
      "Epoch: 850/1000\n",
      "Epoch: 851/1000\n",
      "Epoch: 852/1000\n",
      "Epoch: 853/1000\n",
      "Epoch: 854/1000\n",
      "Epoch: 855/1000\n",
      "Epoch: 856/1000\n",
      "Epoch: 857/1000\n",
      "Epoch: 858/1000\n",
      "Epoch: 859/1000\n",
      "Epoch: 860/1000\n",
      "Epoch: 861/1000\n",
      "Epoch: 862/1000\n",
      "Epoch: 863/1000\n",
      "Epoch: 864/1000\n",
      "Epoch: 865/1000\n",
      "Epoch: 866/1000\n",
      "Epoch: 867/1000\n",
      "Epoch: 868/1000\n",
      "Epoch: 869/1000\n",
      "Epoch: 870/1000\n",
      "Epoch: 871/1000\n",
      "Epoch: 872/1000\n",
      "Epoch: 873/1000\n",
      "Epoch: 874/1000\n",
      "Epoch: 875/1000\n",
      "Epoch: 876/1000\n",
      "Epoch: 877/1000\n",
      "Epoch: 878/1000\n",
      "Epoch: 879/1000\n",
      "Epoch: 880/1000\n",
      "Epoch: 881/1000\n",
      "Epoch: 882/1000\n",
      "Epoch: 883/1000\n",
      "Epoch: 884/1000\n",
      "Epoch: 885/1000\n",
      "Epoch: 886/1000\n",
      "Epoch: 887/1000\n",
      "Epoch: 888/1000\n",
      "Epoch: 889/1000\n",
      "Epoch: 890/1000\n",
      "Epoch: 891/1000\n",
      "Epoch: 892/1000\n",
      "Epoch: 893/1000\n",
      "Epoch: 894/1000\n",
      "Epoch: 895/1000\n",
      "Epoch: 896/1000\n",
      "Epoch: 897/1000\n",
      "Epoch: 898/1000\n",
      "Epoch: 899/1000\n",
      "Epoch: 900/1000\n",
      "Epoch: 901/1000\n",
      "Epoch: 902/1000\n",
      "Epoch: 903/1000\n",
      "Epoch: 904/1000\n",
      "Epoch: 905/1000\n",
      "Epoch: 906/1000\n",
      "Epoch: 907/1000\n",
      "Epoch: 908/1000\n",
      "Epoch: 909/1000\n",
      "Epoch: 910/1000\n",
      "Epoch: 911/1000\n",
      "Epoch: 912/1000\n",
      "Epoch: 913/1000\n",
      "Epoch: 914/1000\n",
      "Epoch: 915/1000\n",
      "Epoch: 916/1000\n",
      "Epoch: 917/1000\n",
      "Epoch: 918/1000\n",
      "Epoch: 919/1000\n",
      "Epoch: 920/1000\n",
      "Epoch: 921/1000\n",
      "Epoch: 922/1000\n",
      "Epoch: 923/1000\n",
      "Epoch: 924/1000\n",
      "Epoch: 925/1000\n",
      "Epoch: 926/1000\n",
      "Epoch: 927/1000\n",
      "Epoch: 928/1000\n",
      "Epoch: 929/1000\n",
      "Epoch: 930/1000\n",
      "Epoch: 931/1000\n",
      "Epoch: 932/1000\n",
      "Epoch: 933/1000\n",
      "Epoch: 934/1000\n",
      "Epoch: 935/1000\n",
      "Epoch: 936/1000\n",
      "Epoch: 937/1000\n",
      "Epoch: 938/1000\n",
      "Epoch: 939/1000\n",
      "Epoch: 940/1000\n",
      "Epoch: 941/1000\n",
      "Epoch: 942/1000\n",
      "Epoch: 943/1000\n",
      "Epoch: 944/1000\n",
      "Epoch: 945/1000\n",
      "Epoch: 946/1000\n",
      "Epoch: 947/1000\n",
      "Epoch: 948/1000\n",
      "Epoch: 949/1000\n",
      "Epoch: 950/1000\n",
      "Epoch: 951/1000\n",
      "Epoch: 952/1000\n",
      "Epoch: 953/1000\n",
      "Epoch: 954/1000\n",
      "Epoch: 955/1000\n",
      "Epoch: 956/1000\n",
      "Epoch: 957/1000\n",
      "Epoch: 958/1000\n",
      "Epoch: 959/1000\n",
      "Epoch: 960/1000\n",
      "Epoch: 961/1000\n",
      "Epoch: 962/1000\n",
      "Epoch: 963/1000\n",
      "Epoch: 964/1000\n",
      "Epoch: 965/1000\n",
      "Epoch: 966/1000\n",
      "Epoch: 967/1000\n",
      "Epoch: 968/1000\n",
      "Epoch: 969/1000\n",
      "Epoch: 970/1000\n",
      "Epoch: 971/1000\n",
      "Epoch: 972/1000\n",
      "Epoch: 973/1000\n",
      "Epoch: 974/1000\n",
      "Epoch: 975/1000\n",
      "Epoch: 976/1000\n",
      "Epoch: 977/1000\n",
      "Epoch: 978/1000\n",
      "Epoch: 979/1000\n",
      "Epoch: 980/1000\n",
      "Epoch: 981/1000\n",
      "Epoch: 982/1000\n",
      "Epoch: 983/1000\n",
      "Epoch: 984/1000\n",
      "Epoch: 985/1000\n",
      "Epoch: 986/1000\n",
      "Epoch: 987/1000\n",
      "Epoch: 988/1000\n",
      "Epoch: 989/1000\n",
      "Epoch: 990/1000\n",
      "Epoch: 991/1000\n",
      "Epoch: 992/1000\n",
      "Epoch: 993/1000\n",
      "Epoch: 994/1000\n",
      "Epoch: 995/1000\n",
      "Epoch: 996/1000\n",
      "Epoch: 997/1000\n",
      "Epoch: 998/1000\n",
      "Epoch: 999/1000\n",
      "Epoch: 1000/1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x24b3239eeb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERFJSP8ff+dK4ZR67KQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24b3239e240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylim([0, 2000])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs * 50')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24b35973da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "batches = np.array_split(range(len(nn.cost_)), 1000)\n",
    "cost_ary = np.array(nn.cost_)\n",
    "cost_avgs = [np.mean(cost_ary[i]) for i in batches]\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='red')\n",
    "plt.ylim([0, 2000])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.71%\n"
     ]
    }
   ],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "y_train_pred = nn.predict(X_train)\n",
    "\n",
    "#if sys.version_info < (3, 0):\n",
    "acc = ((np.sum(y_train == y_train_pred, axis=0)).astype('float') / X_train.shape[0])\n",
    "#acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.03%\n"
     ]
    }
   ],
   "source": [
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "y_test_pred = nn.predict(X_test)\n",
    "\n",
    "\n",
    "acc = ((np.sum(y_test == y_test_pred, axis=0)).astype('float') / X_test.shape[0])\n",
    "#acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

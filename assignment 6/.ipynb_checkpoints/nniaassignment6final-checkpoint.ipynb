{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 6:  Regularization methods in Machine learning and their application in Feedforward neural networks  (deadline: 16 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Regularization methods in ML $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **L2** and **L1** regularization on the weights used for modelling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** is very similar to least squares, except that the weights are estimated by minimizing a slightly different quantity. In particular, the ridge regression co-efficient estimates $\\mathbf{W_{ridge}}$ are the values that minimize, \n",
    "\n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{2}^2}$$ \n",
    "\n",
    "where,\n",
    "\n",
    "$\\mathbf{\\lambda>0}$ is the regularizer,\n",
    "\n",
    "**X** is the design matrix,\n",
    "\n",
    "$\\mathbf{W}$ is the weight vector and\n",
    "\n",
    "**Y** represents the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** seeks weight estimate $W^{Ridge}$ that fit the data well by minimizing the squared error $~$ $\\mathbf{||~Y-XW~||^2}$ (which was also the linear regression cost function).\n",
    "However, the second term, $\\mathbf{||~W~||^2}$, called a ***shrinkage penalty*** is small when $\\mathbf{W}$, i.e., $(w_1, w_2, ..., w_d)^T$ are close to zero. Thus, it has the effect of shrinking the estimates of $w_i$ towards zero.\n",
    "\n",
    "The regularizer $\\mathbf{\\lambda}$ serves to control the relative impact of these two terms on the regression weight estimates. when $\\mathbf{\\lambda=0}$, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\\mathbf{\\lambda \\rightarrow \\infty}$, the impact of the shrinkage penalty grows and the ridge regression weight estimates will approach zero. Unlike least squares, which generates only one set of weight estimates, ridge regression will produce a different set of weight estimates, $\\mathbf{W_{\\lambda}^{Ridge}}$, for each value of $\\mathbf{\\lambda}$. Hence, selecting a good value of $\\mathbf{\\lambda}$ is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ **Plot the magnitude of each weight in $\\mathbf{W^{Ridge}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Ridge weights $\\mathbf{W^{Ridge}}$.** $~$ ($2.5$ points)\n",
    "\n",
    "Download the dataset, **data.csv**, from the NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "# TODO Implement\n",
    "\n",
    "# Read 'Salary' as your response/dependent variable\n",
    "# TODO Implement\n",
    "\n",
    "# Drop the column with the dependent variable 'Salary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize values for the alphas\n",
    "lamdas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# Create a Ridge Object that performs ridge regression\n",
    "# TODO Implement\n",
    "\n",
    "# Create list to hold ridge weights\n",
    "# TODO Implement\n",
    "\n",
    "# Iterate over all lamdas, performing data fitting with ridge regression \n",
    "# and find the corresponding co-efficients\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "\n",
    "#ax.set_xscale('log')\n",
    "#plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Next we deal with **L1 regularization** for which the corresponding method is called **Lasso.** In Lasso, we minimize the function, \n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the magnitude of each weight in $\\mathbf{W^{Lasso}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Lasso weights $\\mathbf{W^{Lasso}}$.** $~$ ($2.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Lasso Object(set max_iter to 10000)\n",
    "# TODO Implement\n",
    "\n",
    "# Create list to hold lasso weights\n",
    "# TODO Implement\n",
    "\n",
    "# Iterate over all alphas, performing data fitting with Lasso\n",
    "# and find the corresponding co-efficients\n",
    "# TODO Implement\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "\n",
    "#ax.set_xscale('log')\n",
    "#plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the plots generated in problems $1$ and $2$ respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Impact of norms in the  Regularizer $~$ (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Assume$~$ $\\mathbf{x} \\in R^2$, $(x_1, x_2) \\in [-1, 1]\\times[-1, 1]$. $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, draw the contour plots for $\\mathbf{\\big|\\big|~x~\\big|\\big|_{0}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{1}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{2}}$ and $\\mathbf{\\big|\\big|~x~\\big|\\big|_{\\infty}}$ norms (consider all possible isolines in the given interval,i.e., ($[-1,1]\\times[-1,1]$) and **explain** how you get the corresponding plot, i.e., provide the mathematical formula for getting the outermost isoline in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Sketch the **Lasso** optimization function, $~$ $\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$ $~$ in two dimensions. From this sketch try to explain why **Lasso** induces **sparsity.** $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know Back-Propagation in details $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://github.com/mmarius/nnia-tutorial/blob/master/neural-net.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a **Feedforward Neural network** with one **input layer**, one **hidden layer** and one **output layer.** The **hidden layer** and **output layer** use the sigmoid function, $\\mathbf{\\sigma(x) = \\frac{1}{1+exp(-x)}}$, as **activation function.** Also note, that the network minimizes **Binary Cross Entropy loss**, given by, $$\\mathbf{J = \\sum -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})}$$\n",
    "\n",
    "We consider the true class labels to be **binary**, i.e., either $0$ or $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the purpose of computing the derivatives of the loss/cost function consider the numerical values obtained by the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer** consists of two nodes, $x_1$ and $x_2$ respectively. For our problem consider the following input,\n",
    "$$\\begin{bmatrix} x_1\\\\ x_2\\\\ \\end{bmatrix} = \\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden layer** is made up of 3 neurons and the corresponding matrix of weights is as given:\n",
    "$$\n",
    "\\mathbf{W_{hidden}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1^{1} & w_1^{2} & w_1^{3} \\\\ w_2^{1} & w_2^{2} & w_2^{3} \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.15 & -0.25 & 0.05\\\\ 0.20 & 0.10 & -0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Output of **Hidden layer** is given by, $~~$ $\\mathbf{a=\\sigma~(W_{hidden}^{T}x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer** consists of one neuron, i.e., the **network** generates a single output. **For our problem, the true class label is $1$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix corresponding to the **Output layer** is given by,\n",
    "$$\n",
    "\\mathbf{W_{out}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1\\\\w_2 \\\\w_3\\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.20\\\\-0.35\\\\0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** output from the **Output layer** is given by, $~$ $\\mathbf{\\hat{y} = \\sigma~(W_{out}^Ta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$6$. Execute the following sequence of operations and **show that Binary Cross Entropy loss is getting reduced, i.e., $ C^2 < C^1$** $~$ ($3$ points)\n",
    "\n",
    "**Perform Forward-propagation to generate output** $\\to$ **Compute loss or cost ($C^1$)** $\\to$ **perform Back-propagation to compute the error** $\\to$ **perform Gradient descent to update the weights** $\\to$ **peform Forward-propagation again with updated weights** $\\to$ **Compute loss or cost ($C^2$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  $C^i$ denotes the loss or cost at the $i^{th}$ iteration, for performing Gradient descent consider a learning rate of $0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11983151]]\n",
      "cost_1 is :  [[ 0.71094875]]\n",
      "cost_2 is :  [[ 0.70631927]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return expit(x)\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def sigmoid_prime(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1.0 - sig)\n",
    "\n",
    "\n",
    "#Input array\n",
    "x=np.array([[-1],[1]])\n",
    "\n",
    "#Output\n",
    "y=1\n",
    "\n",
    "# weights\n",
    "w_hidden = np.array([[0.15, -0.25, 0.05],[0.20, 0.10, -0.15]])\n",
    "w_out = np.array([[0.20],[-0.35],[0.15]])\n",
    "lr = 0.1\n",
    "\n",
    "## forward propagation\n",
    "z1 = w_hidden.T.dot(x)\n",
    "a = sigmoid(z1)\n",
    "z2 = w_out.T.dot(a)\n",
    "y_pred = sigmoid(z2)\n",
    "cost_1 = -y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred)\n",
    "\n",
    "#Backpropagation\n",
    "E = y-y_pred\n",
    "slope_output_layer = sigmoid_prime(y_pred)\n",
    "\n",
    "slope_hidden_layer = sigmoid_prime(a)\n",
    "d_output = E * slope_output_layer * slope_hidden_layer\n",
    "print (d_output )\n",
    "Error_at_hidden_layer = d_output.dot(w_out.T)\n",
    "d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "\n",
    "## gradient descent\n",
    "w_out += y_pred.T.dot(d_output) *lr\n",
    "w_hidden += z1.T.dot(d_hiddenlayer) *lr\n",
    "\n",
    "## forward propagation\n",
    "z1_2 = w_hidden.T.dot(x)\n",
    "a_2 = sigmoid(z1_2)\n",
    "z2_2 = w_out.T.dot(a_2)\n",
    "y_pred_2 = sigmoid(z2_2)\n",
    "\n",
    "cost_2 = -y * np.log(y_pred_2) - (1 - y) * np.log(1 - y_pred_2)\n",
    "\n",
    "#print (\"z1\", z1)\n",
    "#print (\"a\", a)\n",
    "#print (\"z2\", z2)\n",
    "#print (\"pred is : \", y_pred)\n",
    "print (\"cost_1 is : \", cost_1)\n",
    "print (\"cost_2 is : \", cost_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward Neural Network with L1 and L2 regularization $~$ (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using only **Numpy** in python. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L1 and L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **mnist** dataset from NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"function for loading data\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network with a single hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels. \n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float\n",
    "        Regularizer for L1-regularization.\n",
    "        l1=0.0 implies no regularization\n",
    "        \n",
    "    l2 : float\n",
    "        Lambda value for L2-regularization.\n",
    "        l2=0.0 implies no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "        \n",
    "        #TODO Implement\n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output # np.unique(labels)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.w1, self.w2 = self.initialize_weights()\n",
    "\n",
    "\n",
    "    def encode_labels(self, y, k):\n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for i, val in enumerate(y):\n",
    "            onehot[val, i] = 1.0\n",
    "        return onehot\n",
    "        \n",
    "        return one_hot_targets\n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize using random numbers.\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\n",
    "           Implement a stable version which \n",
    "           takes care of overflow and underflow.\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        return expit(z)\n",
    "        \n",
    "\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        sig = self.sigmoid(z)\n",
    "        return sig * (1.0 - sig)\n",
    "        \n",
    "\n",
    "    def add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit to array at index 0\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        \n",
    "        return X_new\n",
    "        \n",
    "        \n",
    "\n",
    "    def feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array,\n",
    "            Input values with bias unit.\n",
    "        z2 : array,\n",
    "            Net input of hidden layer.\n",
    "        a2 : array,\n",
    "            Activation of hidden layer.\n",
    "        z3 : array,\n",
    "            Net input of output layer.\n",
    "        a3 : array,\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        a1 = self.add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        a2 = self.add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self.sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "        #linear_model = self.sigmoid(np.dot(w1, X))\n",
    "        #output = self.sigmoid(np.dot(linear_model, w2))\n",
    "        #return output\n",
    "        \n",
    "\n",
    "    def L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        l2_reg = (lambda_ /2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:] ** 2))\n",
    "        return  l2_reg \n",
    "        \n",
    "        \n",
    "\n",
    "    def L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        l1_reg = (lambda_ /2.0) * (np.abs(w1[:, 1:]).sum() + np.abs(w2[:, 1:]).sum())\n",
    "        return l1_reg\n",
    "        \n",
    "    def get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        \n",
    "        output : array, Activation of the output layer (feedforward)\n",
    "        \n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        l1_reg = self.L1_reg(self.l1, w1, w2)\n",
    "        l2_reg = self.L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + l1_reg + l2_reg\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, Input values with bias unit.\n",
    "        a2 : array, Activation of hidden layer.\n",
    "        a3 : array, Activation of output layer.\n",
    "        z2 : array, Net input of hidden layer.\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        #TODO Implement\n",
    "        output_layer = a3 - y_enc\n",
    "        z2 = self.add_bias_unit(z2, how='row')\n",
    "        hidden_layer = w2.T.dot(output_layer) * self.sigmoid_gradient(z2)\n",
    "        hidden_layer = hidden_layer[1:, :]\n",
    "        grad1 = hidden_layer.dot(a1)\n",
    "        grad2 = output_layer.dot(a2.T)\n",
    "        \n",
    "\n",
    "        # regularize\n",
    "        #TODO Implement\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        \n",
    "        a1, z2, a2, z3, a3 = self.feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "        y : array, Target class labels.\n",
    "        print_progress : bool, Prints the progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #w1=self.w1\n",
    "        #w2=self.w2\n",
    "        #print (w1, w2)\n",
    "        #TODO Implement\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self.encode_labels(y, self.n_output)\n",
    "\n",
    "        w1_prev = np.zeros(self.w1.shape)\n",
    "        w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self.feedforward(X_data[idx], self.w1, self.w2)\n",
    "                cost = self.get_cost(y_enc[:, idx], a3, self.w1, self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # backpropagation\n",
    "                grad1, grad2 = self.get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                d_w1, d_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (d_w1 + (self.eta * w1_prev))\n",
    "                self.w2 -= (d_w2 + (self.eta * w2_prev))\n",
    "                w1_prev, w2_prev = d_w1, d_w2\n",
    "\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = MLP(n_output=10, \n",
    "                  n_features=X_train.shape[1], \n",
    "                  n_hidden=50, \n",
    "                  l2=0.1, \n",
    "                  l1=0.0, \n",
    "                  epochs=1000, \n",
    "                  eta=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  minibatches=50, \n",
    "                  shuffle=True,\n",
    "                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000\n",
      "Epoch: 2/1000\n",
      "Epoch: 3/1000\n",
      "Epoch: 4/1000\n",
      "Epoch: 5/1000\n",
      "Epoch: 6/1000\n",
      "Epoch: 7/1000\n",
      "Epoch: 8/1000\n",
      "Epoch: 9/1000\n",
      "Epoch: 10/1000\n",
      "Epoch: 11/1000\n",
      "Epoch: 12/1000\n",
      "Epoch: 13/1000\n",
      "Epoch: 14/1000\n",
      "Epoch: 15/1000\n",
      "Epoch: 16/1000\n",
      "Epoch: 17/1000\n",
      "Epoch: 18/1000\n",
      "Epoch: 19/1000\n",
      "Epoch: 20/1000\n",
      "Epoch: 21/1000\n",
      "Epoch: 22/1000\n",
      "Epoch: 23/1000\n",
      "Epoch: 24/1000\n",
      "Epoch: 25/1000\n",
      "Epoch: 26/1000\n",
      "Epoch: 27/1000\n",
      "Epoch: 28/1000\n",
      "Epoch: 29/1000\n",
      "Epoch: 30/1000\n",
      "Epoch: 31/1000\n",
      "Epoch: 32/1000\n",
      "Epoch: 33/1000\n",
      "Epoch: 34/1000\n",
      "Epoch: 35/1000\n",
      "Epoch: 36/1000\n",
      "Epoch: 37/1000\n",
      "Epoch: 38/1000\n",
      "Epoch: 39/1000\n",
      "Epoch: 40/1000\n",
      "Epoch: 41/1000\n",
      "Epoch: 42/1000\n",
      "Epoch: 43/1000\n",
      "Epoch: 44/1000\n",
      "Epoch: 45/1000\n",
      "Epoch: 46/1000\n",
      "Epoch: 47/1000\n",
      "Epoch: 48/1000\n",
      "Epoch: 49/1000\n",
      "Epoch: 50/1000\n",
      "Epoch: 51/1000\n",
      "Epoch: 52/1000\n",
      "Epoch: 53/1000\n",
      "Epoch: 54/1000\n",
      "Epoch: 55/1000\n",
      "Epoch: 56/1000\n",
      "Epoch: 57/1000\n",
      "Epoch: 58/1000\n",
      "Epoch: 59/1000\n",
      "Epoch: 60/1000\n",
      "Epoch: 61/1000\n",
      "Epoch: 62/1000\n",
      "Epoch: 63/1000\n",
      "Epoch: 64/1000\n",
      "Epoch: 65/1000\n",
      "Epoch: 66/1000\n",
      "Epoch: 67/1000\n",
      "Epoch: 68/1000\n",
      "Epoch: 69/1000\n",
      "Epoch: 70/1000\n",
      "Epoch: 71/1000\n",
      "Epoch: 72/1000\n",
      "Epoch: 73/1000\n",
      "Epoch: 74/1000\n",
      "Epoch: 75/1000\n",
      "Epoch: 76/1000\n",
      "Epoch: 77/1000\n",
      "Epoch: 78/1000\n",
      "Epoch: 79/1000\n",
      "Epoch: 80/1000\n",
      "Epoch: 81/1000\n",
      "Epoch: 82/1000\n",
      "Epoch: 83/1000\n",
      "Epoch: 84/1000\n",
      "Epoch: 85/1000\n",
      "Epoch: 86/1000\n",
      "Epoch: 87/1000\n",
      "Epoch: 88/1000\n",
      "Epoch: 89/1000\n",
      "Epoch: 90/1000\n",
      "Epoch: 91/1000\n",
      "Epoch: 92/1000\n",
      "Epoch: 93/1000\n",
      "Epoch: 94/1000\n",
      "Epoch: 95/1000\n",
      "Epoch: 96/1000\n",
      "Epoch: 97/1000\n",
      "Epoch: 98/1000\n",
      "Epoch: 99/1000\n",
      "Epoch: 100/1000\n",
      "Epoch: 101/1000\n",
      "Epoch: 102/1000\n",
      "Epoch: 103/1000\n",
      "Epoch: 104/1000\n",
      "Epoch: 105/1000\n",
      "Epoch: 106/1000\n",
      "Epoch: 107/1000\n",
      "Epoch: 108/1000\n",
      "Epoch: 109/1000\n",
      "Epoch: 110/1000\n",
      "Epoch: 111/1000\n",
      "Epoch: 112/1000\n",
      "Epoch: 113/1000\n",
      "Epoch: 114/1000\n",
      "Epoch: 115/1000\n",
      "Epoch: 116/1000\n",
      "Epoch: 117/1000\n",
      "Epoch: 118/1000\n",
      "Epoch: 119/1000\n",
      "Epoch: 120/1000\n",
      "Epoch: 121/1000\n",
      "Epoch: 122/1000\n",
      "Epoch: 123/1000\n",
      "Epoch: 124/1000\n",
      "Epoch: 125/1000\n",
      "Epoch: 126/1000\n",
      "Epoch: 127/1000\n",
      "Epoch: 128/1000\n",
      "Epoch: 129/1000\n",
      "Epoch: 130/1000\n",
      "Epoch: 131/1000\n",
      "Epoch: 132/1000\n",
      "Epoch: 133/1000\n",
      "Epoch: 134/1000\n",
      "Epoch: 135/1000\n",
      "Epoch: 136/1000\n",
      "Epoch: 137/1000\n",
      "Epoch: 138/1000\n",
      "Epoch: 139/1000\n",
      "Epoch: 140/1000\n",
      "Epoch: 141/1000\n",
      "Epoch: 142/1000\n",
      "Epoch: 143/1000\n",
      "Epoch: 144/1000\n",
      "Epoch: 145/1000\n",
      "Epoch: 146/1000\n",
      "Epoch: 147/1000\n",
      "Epoch: 148/1000\n",
      "Epoch: 149/1000\n",
      "Epoch: 150/1000\n",
      "Epoch: 151/1000\n",
      "Epoch: 152/1000\n",
      "Epoch: 153/1000\n",
      "Epoch: 154/1000\n",
      "Epoch: 155/1000\n",
      "Epoch: 156/1000\n",
      "Epoch: 157/1000\n",
      "Epoch: 158/1000\n",
      "Epoch: 159/1000\n",
      "Epoch: 160/1000\n",
      "Epoch: 161/1000\n",
      "Epoch: 162/1000\n",
      "Epoch: 163/1000\n",
      "Epoch: 164/1000\n",
      "Epoch: 165/1000\n",
      "Epoch: 166/1000\n",
      "Epoch: 167/1000\n",
      "Epoch: 168/1000\n",
      "Epoch: 169/1000\n",
      "Epoch: 170/1000\n",
      "Epoch: 171/1000\n",
      "Epoch: 172/1000\n",
      "Epoch: 173/1000\n",
      "Epoch: 174/1000\n",
      "Epoch: 175/1000\n",
      "Epoch: 176/1000\n",
      "Epoch: 177/1000\n",
      "Epoch: 178/1000\n",
      "Epoch: 179/1000\n",
      "Epoch: 180/1000\n",
      "Epoch: 181/1000\n",
      "Epoch: 182/1000\n",
      "Epoch: 183/1000\n",
      "Epoch: 184/1000\n",
      "Epoch: 185/1000\n",
      "Epoch: 186/1000\n",
      "Epoch: 187/1000\n",
      "Epoch: 188/1000\n",
      "Epoch: 189/1000\n",
      "Epoch: 190/1000\n",
      "Epoch: 191/1000\n",
      "Epoch: 192/1000\n",
      "Epoch: 193/1000\n",
      "Epoch: 194/1000\n",
      "Epoch: 195/1000\n",
      "Epoch: 196/1000\n",
      "Epoch: 197/1000\n",
      "Epoch: 198/1000\n",
      "Epoch: 199/1000\n",
      "Epoch: 200/1000\n",
      "Epoch: 201/1000\n",
      "Epoch: 202/1000\n",
      "Epoch: 203/1000\n",
      "Epoch: 204/1000\n",
      "Epoch: 205/1000\n",
      "Epoch: 206/1000\n",
      "Epoch: 207/1000\n",
      "Epoch: 208/1000\n",
      "Epoch: 209/1000\n",
      "Epoch: 210/1000\n",
      "Epoch: 211/1000\n",
      "Epoch: 212/1000\n",
      "Epoch: 213/1000\n",
      "Epoch: 214/1000\n",
      "Epoch: 215/1000\n",
      "Epoch: 216/1000\n",
      "Epoch: 217/1000\n",
      "Epoch: 218/1000\n",
      "Epoch: 219/1000\n",
      "Epoch: 220/1000\n",
      "Epoch: 221/1000\n",
      "Epoch: 222/1000\n",
      "Epoch: 223/1000\n",
      "Epoch: 224/1000\n",
      "Epoch: 225/1000\n",
      "Epoch: 226/1000\n",
      "Epoch: 227/1000\n",
      "Epoch: 228/1000\n",
      "Epoch: 229/1000\n",
      "Epoch: 230/1000\n",
      "Epoch: 231/1000\n",
      "Epoch: 232/1000\n",
      "Epoch: 233/1000\n",
      "Epoch: 234/1000\n",
      "Epoch: 235/1000\n",
      "Epoch: 236/1000\n",
      "Epoch: 237/1000\n",
      "Epoch: 238/1000\n",
      "Epoch: 239/1000\n",
      "Epoch: 240/1000\n",
      "Epoch: 241/1000\n",
      "Epoch: 242/1000\n",
      "Epoch: 243/1000\n",
      "Epoch: 244/1000\n",
      "Epoch: 245/1000\n",
      "Epoch: 246/1000\n",
      "Epoch: 247/1000\n",
      "Epoch: 248/1000\n",
      "Epoch: 249/1000\n",
      "Epoch: 250/1000\n",
      "Epoch: 251/1000\n",
      "Epoch: 252/1000\n",
      "Epoch: 253/1000\n",
      "Epoch: 254/1000\n",
      "Epoch: 255/1000\n",
      "Epoch: 256/1000\n",
      "Epoch: 257/1000\n",
      "Epoch: 258/1000\n",
      "Epoch: 259/1000\n",
      "Epoch: 260/1000\n",
      "Epoch: 261/1000\n",
      "Epoch: 262/1000\n",
      "Epoch: 263/1000\n",
      "Epoch: 264/1000\n",
      "Epoch: 265/1000\n",
      "Epoch: 266/1000\n",
      "Epoch: 267/1000\n",
      "Epoch: 268/1000\n",
      "Epoch: 269/1000\n",
      "Epoch: 270/1000\n",
      "Epoch: 271/1000\n",
      "Epoch: 272/1000\n",
      "Epoch: 273/1000\n",
      "Epoch: 274/1000\n",
      "Epoch: 275/1000\n",
      "Epoch: 276/1000\n",
      "Epoch: 277/1000\n",
      "Epoch: 278/1000\n",
      "Epoch: 279/1000\n",
      "Epoch: 280/1000\n",
      "Epoch: 281/1000\n",
      "Epoch: 282/1000\n",
      "Epoch: 283/1000\n",
      "Epoch: 284/1000\n",
      "Epoch: 285/1000\n",
      "Epoch: 286/1000\n",
      "Epoch: 287/1000\n",
      "Epoch: 288/1000\n",
      "Epoch: 289/1000\n",
      "Epoch: 290/1000\n",
      "Epoch: 291/1000\n",
      "Epoch: 292/1000\n",
      "Epoch: 293/1000\n",
      "Epoch: 294/1000\n",
      "Epoch: 295/1000\n",
      "Epoch: 296/1000\n",
      "Epoch: 297/1000\n",
      "Epoch: 298/1000\n",
      "Epoch: 299/1000\n",
      "Epoch: 300/1000\n",
      "Epoch: 301/1000\n",
      "Epoch: 302/1000\n",
      "Epoch: 303/1000\n",
      "Epoch: 304/1000\n",
      "Epoch: 305/1000\n",
      "Epoch: 306/1000\n",
      "Epoch: 307/1000\n",
      "Epoch: 308/1000\n",
      "Epoch: 309/1000\n",
      "Epoch: 310/1000\n",
      "Epoch: 311/1000\n",
      "Epoch: 312/1000\n",
      "Epoch: 313/1000\n",
      "Epoch: 314/1000\n",
      "Epoch: 315/1000\n",
      "Epoch: 316/1000\n",
      "Epoch: 317/1000\n",
      "Epoch: 318/1000\n",
      "Epoch: 319/1000\n",
      "Epoch: 320/1000\n",
      "Epoch: 321/1000\n",
      "Epoch: 322/1000\n",
      "Epoch: 323/1000\n",
      "Epoch: 324/1000\n",
      "Epoch: 325/1000\n",
      "Epoch: 326/1000\n",
      "Epoch: 327/1000\n",
      "Epoch: 328/1000\n",
      "Epoch: 329/1000\n",
      "Epoch: 330/1000\n",
      "Epoch: 331/1000\n",
      "Epoch: 332/1000\n",
      "Epoch: 333/1000\n",
      "Epoch: 334/1000\n",
      "Epoch: 335/1000\n",
      "Epoch: 336/1000\n",
      "Epoch: 337/1000\n",
      "Epoch: 338/1000\n",
      "Epoch: 339/1000\n",
      "Epoch: 340/1000\n",
      "Epoch: 341/1000\n",
      "Epoch: 342/1000\n",
      "Epoch: 343/1000\n",
      "Epoch: 344/1000\n",
      "Epoch: 345/1000\n",
      "Epoch: 346/1000\n",
      "Epoch: 347/1000\n",
      "Epoch: 348/1000\n",
      "Epoch: 349/1000\n",
      "Epoch: 350/1000\n",
      "Epoch: 351/1000\n",
      "Epoch: 352/1000\n",
      "Epoch: 353/1000\n",
      "Epoch: 354/1000\n",
      "Epoch: 355/1000\n",
      "Epoch: 356/1000\n",
      "Epoch: 357/1000\n",
      "Epoch: 358/1000\n",
      "Epoch: 359/1000\n",
      "Epoch: 360/1000\n",
      "Epoch: 361/1000\n",
      "Epoch: 362/1000\n",
      "Epoch: 363/1000\n",
      "Epoch: 364/1000\n",
      "Epoch: 365/1000\n",
      "Epoch: 366/1000\n",
      "Epoch: 367/1000\n",
      "Epoch: 368/1000\n",
      "Epoch: 369/1000\n",
      "Epoch: 370/1000\n",
      "Epoch: 371/1000\n",
      "Epoch: 372/1000\n",
      "Epoch: 373/1000\n",
      "Epoch: 374/1000\n",
      "Epoch: 375/1000\n",
      "Epoch: 376/1000\n",
      "Epoch: 377/1000\n",
      "Epoch: 378/1000\n",
      "Epoch: 379/1000\n",
      "Epoch: 380/1000\n",
      "Epoch: 381/1000\n",
      "Epoch: 382/1000\n",
      "Epoch: 383/1000\n",
      "Epoch: 384/1000\n",
      "Epoch: 385/1000\n",
      "Epoch: 386/1000\n",
      "Epoch: 387/1000\n",
      "Epoch: 388/1000\n",
      "Epoch: 389/1000\n",
      "Epoch: 390/1000\n",
      "Epoch: 391/1000\n",
      "Epoch: 392/1000\n",
      "Epoch: 393/1000\n",
      "Epoch: 394/1000\n",
      "Epoch: 395/1000\n",
      "Epoch: 396/1000\n",
      "Epoch: 397/1000\n",
      "Epoch: 398/1000\n",
      "Epoch: 399/1000\n",
      "Epoch: 400/1000\n",
      "Epoch: 401/1000\n",
      "Epoch: 402/1000\n",
      "Epoch: 403/1000\n",
      "Epoch: 404/1000\n",
      "Epoch: 405/1000\n",
      "Epoch: 406/1000\n",
      "Epoch: 407/1000\n",
      "Epoch: 408/1000\n",
      "Epoch: 409/1000\n",
      "Epoch: 410/1000\n",
      "Epoch: 411/1000\n",
      "Epoch: 412/1000\n",
      "Epoch: 413/1000\n",
      "Epoch: 414/1000\n",
      "Epoch: 415/1000\n",
      "Epoch: 416/1000\n",
      "Epoch: 417/1000\n",
      "Epoch: 418/1000\n",
      "Epoch: 419/1000\n",
      "Epoch: 420/1000\n",
      "Epoch: 421/1000\n",
      "Epoch: 422/1000\n",
      "Epoch: 423/1000\n",
      "Epoch: 424/1000\n",
      "Epoch: 425/1000\n",
      "Epoch: 426/1000\n",
      "Epoch: 427/1000\n",
      "Epoch: 428/1000\n",
      "Epoch: 429/1000\n",
      "Epoch: 430/1000\n",
      "Epoch: 431/1000\n",
      "Epoch: 432/1000\n",
      "Epoch: 433/1000\n",
      "Epoch: 434/1000\n",
      "Epoch: 435/1000\n",
      "Epoch: 436/1000\n",
      "Epoch: 437/1000\n",
      "Epoch: 438/1000\n",
      "Epoch: 439/1000\n",
      "Epoch: 440/1000\n",
      "Epoch: 441/1000\n",
      "Epoch: 442/1000\n",
      "Epoch: 443/1000\n",
      "Epoch: 444/1000\n",
      "Epoch: 445/1000\n",
      "Epoch: 446/1000\n",
      "Epoch: 447/1000\n",
      "Epoch: 448/1000\n",
      "Epoch: 449/1000\n",
      "Epoch: 450/1000\n",
      "Epoch: 451/1000\n",
      "Epoch: 452/1000\n",
      "Epoch: 453/1000\n",
      "Epoch: 454/1000\n",
      "Epoch: 455/1000\n",
      "Epoch: 456/1000\n",
      "Epoch: 457/1000\n",
      "Epoch: 458/1000\n",
      "Epoch: 459/1000\n",
      "Epoch: 460/1000\n",
      "Epoch: 461/1000\n",
      "Epoch: 462/1000\n",
      "Epoch: 463/1000\n",
      "Epoch: 464/1000\n",
      "Epoch: 465/1000\n",
      "Epoch: 466/1000\n",
      "Epoch: 467/1000\n",
      "Epoch: 468/1000\n",
      "Epoch: 469/1000\n",
      "Epoch: 470/1000\n",
      "Epoch: 471/1000\n",
      "Epoch: 472/1000\n",
      "Epoch: 473/1000\n",
      "Epoch: 474/1000\n",
      "Epoch: 475/1000\n",
      "Epoch: 476/1000\n",
      "Epoch: 477/1000\n",
      "Epoch: 478/1000\n",
      "Epoch: 479/1000\n",
      "Epoch: 480/1000\n",
      "Epoch: 481/1000\n",
      "Epoch: 482/1000\n",
      "Epoch: 483/1000\n",
      "Epoch: 484/1000\n",
      "Epoch: 485/1000\n",
      "Epoch: 486/1000\n",
      "Epoch: 487/1000\n",
      "Epoch: 488/1000\n",
      "Epoch: 489/1000\n",
      "Epoch: 490/1000\n",
      "Epoch: 491/1000\n",
      "Epoch: 492/1000\n",
      "Epoch: 493/1000\n",
      "Epoch: 494/1000\n",
      "Epoch: 495/1000\n",
      "Epoch: 496/1000\n",
      "Epoch: 497/1000\n",
      "Epoch: 498/1000\n",
      "Epoch: 499/1000\n",
      "Epoch: 500/1000\n",
      "Epoch: 501/1000\n",
      "Epoch: 502/1000\n",
      "Epoch: 503/1000\n",
      "Epoch: 504/1000\n",
      "Epoch: 505/1000\n",
      "Epoch: 506/1000\n",
      "Epoch: 507/1000\n",
      "Epoch: 508/1000\n",
      "Epoch: 509/1000\n",
      "Epoch: 510/1000\n",
      "Epoch: 511/1000\n",
      "Epoch: 512/1000\n",
      "Epoch: 513/1000\n",
      "Epoch: 514/1000\n",
      "Epoch: 515/1000\n",
      "Epoch: 516/1000\n",
      "Epoch: 517/1000\n",
      "Epoch: 518/1000\n",
      "Epoch: 519/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 520/1000\n",
      "Epoch: 521/1000\n",
      "Epoch: 522/1000\n",
      "Epoch: 523/1000\n",
      "Epoch: 524/1000\n",
      "Epoch: 525/1000\n",
      "Epoch: 526/1000\n",
      "Epoch: 527/1000\n",
      "Epoch: 528/1000\n",
      "Epoch: 529/1000\n",
      "Epoch: 530/1000\n",
      "Epoch: 531/1000\n",
      "Epoch: 532/1000\n",
      "Epoch: 533/1000\n",
      "Epoch: 534/1000\n",
      "Epoch: 535/1000\n",
      "Epoch: 536/1000\n",
      "Epoch: 537/1000\n",
      "Epoch: 538/1000\n",
      "Epoch: 539/1000\n",
      "Epoch: 540/1000\n",
      "Epoch: 541/1000\n",
      "Epoch: 542/1000\n",
      "Epoch: 543/1000\n",
      "Epoch: 544/1000\n",
      "Epoch: 545/1000\n",
      "Epoch: 546/1000\n",
      "Epoch: 547/1000\n",
      "Epoch: 548/1000\n",
      "Epoch: 549/1000\n",
      "Epoch: 550/1000\n",
      "Epoch: 551/1000\n",
      "Epoch: 552/1000\n",
      "Epoch: 553/1000\n",
      "Epoch: 554/1000\n",
      "Epoch: 555/1000\n",
      "Epoch: 556/1000\n",
      "Epoch: 557/1000\n",
      "Epoch: 558/1000\n",
      "Epoch: 559/1000\n",
      "Epoch: 560/1000\n",
      "Epoch: 561/1000\n",
      "Epoch: 562/1000\n",
      "Epoch: 563/1000\n",
      "Epoch: 564/1000\n",
      "Epoch: 565/1000\n",
      "Epoch: 566/1000\n",
      "Epoch: 567/1000\n",
      "Epoch: 568/1000\n",
      "Epoch: 569/1000\n",
      "Epoch: 570/1000\n",
      "Epoch: 571/1000\n",
      "Epoch: 572/1000\n",
      "Epoch: 573/1000\n",
      "Epoch: 574/1000\n",
      "Epoch: 575/1000\n",
      "Epoch: 576/1000\n",
      "Epoch: 577/1000\n",
      "Epoch: 578/1000\n",
      "Epoch: 579/1000\n",
      "Epoch: 580/1000\n",
      "Epoch: 581/1000\n",
      "Epoch: 582/1000\n",
      "Epoch: 583/1000\n",
      "Epoch: 584/1000\n",
      "Epoch: 585/1000\n",
      "Epoch: 586/1000\n",
      "Epoch: 587/1000\n",
      "Epoch: 588/1000\n",
      "Epoch: 589/1000\n",
      "Epoch: 590/1000\n",
      "Epoch: 591/1000\n",
      "Epoch: 592/1000\n",
      "Epoch: 593/1000\n",
      "Epoch: 594/1000\n",
      "Epoch: 595/1000\n",
      "Epoch: 596/1000\n",
      "Epoch: 597/1000\n",
      "Epoch: 598/1000\n",
      "Epoch: 599/1000\n",
      "Epoch: 600/1000\n",
      "Epoch: 601/1000\n",
      "Epoch: 602/1000\n",
      "Epoch: 603/1000\n",
      "Epoch: 604/1000\n",
      "Epoch: 605/1000\n",
      "Epoch: 606/1000\n",
      "Epoch: 607/1000\n",
      "Epoch: 608/1000\n",
      "Epoch: 609/1000\n",
      "Epoch: 610/1000\n",
      "Epoch: 611/1000\n",
      "Epoch: 612/1000\n",
      "Epoch: 613/1000\n",
      "Epoch: 614/1000\n",
      "Epoch: 615/1000\n",
      "Epoch: 616/1000\n",
      "Epoch: 617/1000\n",
      "Epoch: 618/1000\n",
      "Epoch: 619/1000\n",
      "Epoch: 620/1000\n",
      "Epoch: 621/1000\n",
      "Epoch: 622/1000\n",
      "Epoch: 623/1000\n",
      "Epoch: 624/1000\n",
      "Epoch: 625/1000\n",
      "Epoch: 626/1000\n",
      "Epoch: 627/1000\n",
      "Epoch: 628/1000\n",
      "Epoch: 629/1000\n",
      "Epoch: 630/1000\n",
      "Epoch: 631/1000\n",
      "Epoch: 632/1000\n",
      "Epoch: 633/1000\n",
      "Epoch: 634/1000\n",
      "Epoch: 635/1000\n",
      "Epoch: 636/1000\n",
      "Epoch: 637/1000\n",
      "Epoch: 638/1000\n",
      "Epoch: 639/1000\n",
      "Epoch: 640/1000\n",
      "Epoch: 641/1000\n",
      "Epoch: 642/1000\n",
      "Epoch: 643/1000\n",
      "Epoch: 644/1000\n",
      "Epoch: 645/1000\n",
      "Epoch: 646/1000\n",
      "Epoch: 647/1000\n",
      "Epoch: 648/1000\n",
      "Epoch: 649/1000\n",
      "Epoch: 650/1000\n",
      "Epoch: 651/1000\n",
      "Epoch: 652/1000\n",
      "Epoch: 653/1000\n",
      "Epoch: 654/1000\n",
      "Epoch: 655/1000\n",
      "Epoch: 656/1000\n",
      "Epoch: 657/1000\n",
      "Epoch: 658/1000\n",
      "Epoch: 659/1000\n",
      "Epoch: 660/1000\n",
      "Epoch: 661/1000\n",
      "Epoch: 662/1000\n",
      "Epoch: 663/1000\n",
      "Epoch: 664/1000\n",
      "Epoch: 665/1000\n",
      "Epoch: 666/1000\n",
      "Epoch: 667/1000\n",
      "Epoch: 668/1000\n",
      "Epoch: 669/1000\n",
      "Epoch: 670/1000\n",
      "Epoch: 671/1000\n",
      "Epoch: 672/1000\n",
      "Epoch: 673/1000\n",
      "Epoch: 674/1000\n",
      "Epoch: 675/1000\n",
      "Epoch: 676/1000\n",
      "Epoch: 677/1000\n",
      "Epoch: 678/1000\n",
      "Epoch: 679/1000\n",
      "Epoch: 680/1000\n",
      "Epoch: 681/1000\n",
      "Epoch: 682/1000\n",
      "Epoch: 683/1000\n",
      "Epoch: 684/1000\n",
      "Epoch: 685/1000\n",
      "Epoch: 686/1000\n",
      "Epoch: 687/1000\n",
      "Epoch: 688/1000\n",
      "Epoch: 689/1000\n",
      "Epoch: 690/1000\n",
      "Epoch: 691/1000\n",
      "Epoch: 692/1000\n",
      "Epoch: 693/1000\n",
      "Epoch: 694/1000\n",
      "Epoch: 695/1000\n",
      "Epoch: 696/1000\n",
      "Epoch: 697/1000\n",
      "Epoch: 698/1000\n",
      "Epoch: 699/1000\n",
      "Epoch: 700/1000\n",
      "Epoch: 701/1000\n",
      "Epoch: 702/1000\n",
      "Epoch: 703/1000\n",
      "Epoch: 704/1000\n",
      "Epoch: 705/1000\n",
      "Epoch: 706/1000\n",
      "Epoch: 707/1000\n",
      "Epoch: 708/1000\n",
      "Epoch: 709/1000\n",
      "Epoch: 710/1000\n",
      "Epoch: 711/1000\n",
      "Epoch: 712/1000\n",
      "Epoch: 713/1000\n",
      "Epoch: 714/1000\n",
      "Epoch: 715/1000\n",
      "Epoch: 716/1000\n",
      "Epoch: 717/1000\n",
      "Epoch: 718/1000\n",
      "Epoch: 719/1000\n",
      "Epoch: 720/1000\n",
      "Epoch: 721/1000\n",
      "Epoch: 722/1000\n",
      "Epoch: 723/1000\n",
      "Epoch: 724/1000\n",
      "Epoch: 725/1000\n",
      "Epoch: 726/1000\n",
      "Epoch: 727/1000\n",
      "Epoch: 728/1000\n",
      "Epoch: 729/1000\n",
      "Epoch: 730/1000\n",
      "Epoch: 731/1000\n",
      "Epoch: 732/1000\n",
      "Epoch: 733/1000\n",
      "Epoch: 734/1000\n",
      "Epoch: 735/1000\n",
      "Epoch: 736/1000\n",
      "Epoch: 737/1000\n",
      "Epoch: 738/1000\n",
      "Epoch: 739/1000\n",
      "Epoch: 740/1000\n",
      "Epoch: 741/1000\n",
      "Epoch: 742/1000\n",
      "Epoch: 743/1000\n",
      "Epoch: 744/1000\n",
      "Epoch: 745/1000\n",
      "Epoch: 746/1000\n",
      "Epoch: 747/1000\n",
      "Epoch: 748/1000\n",
      "Epoch: 749/1000\n",
      "Epoch: 750/1000\n",
      "Epoch: 751/1000\n",
      "Epoch: 752/1000\n",
      "Epoch: 753/1000\n",
      "Epoch: 754/1000\n",
      "Epoch: 755/1000\n",
      "Epoch: 756/1000\n",
      "Epoch: 757/1000\n",
      "Epoch: 758/1000\n",
      "Epoch: 759/1000\n",
      "Epoch: 760/1000\n",
      "Epoch: 761/1000\n",
      "Epoch: 762/1000\n",
      "Epoch: 763/1000\n",
      "Epoch: 764/1000\n",
      "Epoch: 765/1000\n",
      "Epoch: 766/1000\n",
      "Epoch: 767/1000\n",
      "Epoch: 768/1000\n",
      "Epoch: 769/1000\n",
      "Epoch: 770/1000\n",
      "Epoch: 771/1000\n",
      "Epoch: 772/1000\n",
      "Epoch: 773/1000\n",
      "Epoch: 774/1000\n",
      "Epoch: 775/1000\n",
      "Epoch: 776/1000\n",
      "Epoch: 777/1000\n",
      "Epoch: 778/1000\n",
      "Epoch: 779/1000\n",
      "Epoch: 780/1000\n",
      "Epoch: 781/1000\n",
      "Epoch: 782/1000\n",
      "Epoch: 783/1000\n",
      "Epoch: 784/1000\n",
      "Epoch: 785/1000\n",
      "Epoch: 786/1000\n",
      "Epoch: 787/1000\n",
      "Epoch: 788/1000\n",
      "Epoch: 789/1000\n",
      "Epoch: 790/1000\n",
      "Epoch: 791/1000\n",
      "Epoch: 792/1000\n",
      "Epoch: 793/1000\n",
      "Epoch: 794/1000\n",
      "Epoch: 795/1000\n",
      "Epoch: 796/1000\n",
      "Epoch: 797/1000\n",
      "Epoch: 798/1000\n",
      "Epoch: 799/1000\n",
      "Epoch: 800/1000\n",
      "Epoch: 801/1000\n",
      "Epoch: 802/1000\n",
      "Epoch: 803/1000\n",
      "Epoch: 804/1000\n",
      "Epoch: 805/1000\n",
      "Epoch: 806/1000\n",
      "Epoch: 807/1000\n",
      "Epoch: 808/1000\n",
      "Epoch: 809/1000\n",
      "Epoch: 810/1000\n",
      "Epoch: 811/1000\n",
      "Epoch: 812/1000\n",
      "Epoch: 813/1000\n",
      "Epoch: 814/1000\n",
      "Epoch: 815/1000\n",
      "Epoch: 816/1000\n",
      "Epoch: 817/1000\n",
      "Epoch: 818/1000\n",
      "Epoch: 819/1000\n",
      "Epoch: 820/1000\n",
      "Epoch: 821/1000\n",
      "Epoch: 822/1000\n",
      "Epoch: 823/1000\n",
      "Epoch: 824/1000\n",
      "Epoch: 825/1000\n",
      "Epoch: 826/1000\n",
      "Epoch: 827/1000\n",
      "Epoch: 828/1000\n",
      "Epoch: 829/1000\n",
      "Epoch: 830/1000\n",
      "Epoch: 831/1000\n",
      "Epoch: 832/1000\n",
      "Epoch: 833/1000\n",
      "Epoch: 834/1000\n",
      "Epoch: 835/1000\n",
      "Epoch: 836/1000\n",
      "Epoch: 837/1000\n",
      "Epoch: 838/1000\n",
      "Epoch: 839/1000\n",
      "Epoch: 840/1000\n",
      "Epoch: 841/1000\n",
      "Epoch: 842/1000\n",
      "Epoch: 843/1000\n",
      "Epoch: 844/1000\n",
      "Epoch: 845/1000\n",
      "Epoch: 846/1000\n",
      "Epoch: 847/1000\n",
      "Epoch: 848/1000\n",
      "Epoch: 849/1000\n",
      "Epoch: 850/1000\n",
      "Epoch: 851/1000\n",
      "Epoch: 852/1000\n",
      "Epoch: 853/1000\n",
      "Epoch: 854/1000\n",
      "Epoch: 855/1000\n",
      "Epoch: 856/1000\n",
      "Epoch: 857/1000\n",
      "Epoch: 858/1000\n",
      "Epoch: 859/1000\n",
      "Epoch: 860/1000\n",
      "Epoch: 861/1000\n",
      "Epoch: 862/1000\n",
      "Epoch: 863/1000\n",
      "Epoch: 864/1000\n",
      "Epoch: 865/1000\n",
      "Epoch: 866/1000\n",
      "Epoch: 867/1000\n",
      "Epoch: 868/1000\n",
      "Epoch: 869/1000\n",
      "Epoch: 870/1000\n",
      "Epoch: 871/1000\n",
      "Epoch: 872/1000\n",
      "Epoch: 873/1000\n",
      "Epoch: 874/1000\n",
      "Epoch: 875/1000\n",
      "Epoch: 876/1000\n",
      "Epoch: 877/1000\n",
      "Epoch: 878/1000\n",
      "Epoch: 879/1000\n",
      "Epoch: 880/1000\n",
      "Epoch: 881/1000\n",
      "Epoch: 882/1000\n",
      "Epoch: 883/1000\n",
      "Epoch: 884/1000\n",
      "Epoch: 885/1000\n",
      "Epoch: 886/1000\n",
      "Epoch: 887/1000\n",
      "Epoch: 888/1000\n",
      "Epoch: 889/1000\n",
      "Epoch: 890/1000\n",
      "Epoch: 891/1000\n",
      "Epoch: 892/1000\n",
      "Epoch: 893/1000\n",
      "Epoch: 894/1000\n",
      "Epoch: 895/1000\n",
      "Epoch: 896/1000\n",
      "Epoch: 897/1000\n",
      "Epoch: 898/1000\n",
      "Epoch: 899/1000\n",
      "Epoch: 900/1000\n",
      "Epoch: 901/1000\n",
      "Epoch: 902/1000\n",
      "Epoch: 903/1000\n",
      "Epoch: 904/1000\n",
      "Epoch: 905/1000\n",
      "Epoch: 906/1000\n",
      "Epoch: 907/1000\n",
      "Epoch: 908/1000\n",
      "Epoch: 909/1000\n",
      "Epoch: 910/1000\n",
      "Epoch: 911/1000\n",
      "Epoch: 912/1000\n",
      "Epoch: 913/1000\n",
      "Epoch: 914/1000\n",
      "Epoch: 915/1000\n",
      "Epoch: 916/1000\n",
      "Epoch: 917/1000\n",
      "Epoch: 918/1000\n",
      "Epoch: 919/1000\n",
      "Epoch: 920/1000\n",
      "Epoch: 921/1000\n",
      "Epoch: 922/1000\n",
      "Epoch: 923/1000\n",
      "Epoch: 924/1000\n",
      "Epoch: 925/1000\n",
      "Epoch: 926/1000\n",
      "Epoch: 927/1000\n",
      "Epoch: 928/1000\n",
      "Epoch: 929/1000\n",
      "Epoch: 930/1000\n",
      "Epoch: 931/1000\n",
      "Epoch: 932/1000\n",
      "Epoch: 933/1000\n",
      "Epoch: 934/1000\n",
      "Epoch: 935/1000\n",
      "Epoch: 936/1000\n",
      "Epoch: 937/1000\n",
      "Epoch: 938/1000\n",
      "Epoch: 939/1000\n",
      "Epoch: 940/1000\n",
      "Epoch: 941/1000\n",
      "Epoch: 942/1000\n",
      "Epoch: 943/1000\n",
      "Epoch: 944/1000\n",
      "Epoch: 945/1000\n",
      "Epoch: 946/1000\n",
      "Epoch: 947/1000\n",
      "Epoch: 948/1000\n",
      "Epoch: 949/1000\n",
      "Epoch: 950/1000\n",
      "Epoch: 951/1000\n",
      "Epoch: 952/1000\n",
      "Epoch: 953/1000\n",
      "Epoch: 954/1000\n",
      "Epoch: 955/1000\n",
      "Epoch: 956/1000\n",
      "Epoch: 957/1000\n",
      "Epoch: 958/1000\n",
      "Epoch: 959/1000\n",
      "Epoch: 960/1000\n",
      "Epoch: 961/1000\n",
      "Epoch: 962/1000\n",
      "Epoch: 963/1000\n",
      "Epoch: 964/1000\n",
      "Epoch: 965/1000\n",
      "Epoch: 966/1000\n",
      "Epoch: 967/1000\n",
      "Epoch: 968/1000\n",
      "Epoch: 969/1000\n",
      "Epoch: 970/1000\n",
      "Epoch: 971/1000\n",
      "Epoch: 972/1000\n",
      "Epoch: 973/1000\n",
      "Epoch: 974/1000\n",
      "Epoch: 975/1000\n",
      "Epoch: 976/1000\n",
      "Epoch: 977/1000\n",
      "Epoch: 978/1000\n",
      "Epoch: 979/1000\n",
      "Epoch: 980/1000\n",
      "Epoch: 981/1000\n",
      "Epoch: 982/1000\n",
      "Epoch: 983/1000\n",
      "Epoch: 984/1000\n",
      "Epoch: 985/1000\n",
      "Epoch: 986/1000\n",
      "Epoch: 987/1000\n",
      "Epoch: 988/1000\n",
      "Epoch: 989/1000\n",
      "Epoch: 990/1000\n",
      "Epoch: 991/1000\n",
      "Epoch: 992/1000\n",
      "Epoch: 993/1000\n",
      "Epoch: 994/1000\n",
      "Epoch: 995/1000\n",
      "Epoch: 996/1000\n",
      "Epoch: 997/1000\n",
      "Epoch: 998/1000\n",
      "Epoch: 999/1000\n",
      "Epoch: 1000/1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x23ccc4c3ac8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23ccc50eeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylim([0, 2000])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs * 50')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23ccc4e1240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "batches = np.array_split(range(len(nn.cost_)), 1000)\n",
    "cost_ary = np.array(nn.cost_)\n",
    "cost_avgs = [np.mean(cost_ary[i]) for i in batches]\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='red')\n",
    "plt.ylim([0, 2000])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.89%\n"
     ]
    }
   ],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "y_train_pred = nn.predict(X_train)\n",
    "\n",
    "#if sys.version_info < (3, 0):\n",
    "acc = ((np.sum(y_train == y_train_pred, axis=0)).astype('float') / X_train.shape[0])\n",
    "#acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.06%\n"
     ]
    }
   ],
   "source": [
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "y_test_pred = nn.predict(X_test)\n",
    "\n",
    "\n",
    "acc = ((np.sum(y_test == y_test_pred, axis=0)).astype('float') / X_test.shape[0])\n",
    "#acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 5: Classification with TensorFlow (deadline: 8 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Practicing Chain-Rule and Backpropagation (5.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following function $$ f(w, b) = \\frac{e^{w+b} w + \\sigma(w) + b}{(\\sigma(b) + \\sigma(w))Â²}$$ with $\\sigma(x) = \\frac{1}{1+e^{-x}}$ and $w, b \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a)$ Compute the partial derivatives of $f(w, b)$ with respect to $w$ and $b$ analytically. ($0.5$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b)$ Decompose the function $f(w, b)$ into primitives. Rewrite the function using these primitives. ($0.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Write function primitives in python\n",
    "\n",
    "# Example\n",
    "def square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c)$ Draw a computation graph for the function $f(w, b)$ using the primitives defined above as nodes. You may want to use the [NetworkX](https://networkx.github.io/) library for this task. You can install it by running `conda install networkx` from within your activated *nnia* conda environment. ($1.0$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d)$ Specify $\\frac{\\partial}{\\partial x} p_i$ for each of the primitives $p_i$. ($1.0$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Specifiy derivatives of the primitives in python\n",
    "\n",
    "# Example\n",
    "def dsquare(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$e)$ Given the inputs $w = 5, b = -3$. Compute the forward pass of your function. You might want to store the intermediate result in a dictionary in order to later use them in backward pass. ($1.0$ point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the forward pass using the primitives defined above\n",
    "def forward(w, b):\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f)$ For the same inputs as above, perform the backward pass in order to compute $\\frac{\\partial}{\\partial w}f~\\mid_{w=5, b=-3}$ and $\\frac{\\partial}{\\partial b}f~\\mid_{w=5, b=-3}$. ($1.0$ point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the backward pass using the derivatives of the primitives defined above. \n",
    "def backward(w, b):\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification using Feedforward Neural Networks with TensorFlow (12.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a feedforward neural network using TensorFlow. We will then train the neural network on a simple two class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some dummy data using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data\n",
    "X, y = make_moons(n_samples= 300, noise=0.15, random_state=123)\n",
    "\n",
    "# Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1)\n",
    "y_train = np.reshape(y_train, newshape=(-1, 1))\n",
    "y_test = np.reshape(y_test, newshape=(-1, 1))\n",
    "\n",
    "# Plot the training data and color the data point according to their class label\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "axes.scatter(X_train[: , 0], X_train[: , 1], c=y_train.flatten(), cmap='Set1')\n",
    "axes.set_title('Training Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following you are allowed to use predefined functions of the TensorFlow API such as `tf.get_variable()`, `tf.nn.relu(x)` or `tf.random_normal_initializer()`. Addtionally you are allowed to use the gradient descent optimizer `tf.train.GradientDescentOptimizer` predefined by TensorFlow. However, you are not allowed to use any predefined layer from the [API](https://www.tensorflow.org/api_docs/python/tf/layers) such as `tf.Layers.Dense` or `tf.contrib.layers.fully_connected`. It is your task to implement these layers yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(x)$ is the so called hidden layer, and $f^{(2)}(x)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that you will implement in this exercise has the following layers:\n",
    "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$ where $b$ is the batch size and $p$ is the number of features.\n",
    "* $f^{(1)}(x) = g(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $b_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **units** of this particular layer. Additonally, $g(x) = \\max{\\{0, x\\}}$ is the so called **rectified linear unit** (*ReLU*) activation function. \n",
    "* $f^{(2)}(x) = \\sigma(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_1} \\in \\mathbb{R}^{u_1,u_2}$, $b_1 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **units** of this particular layer. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{-x}}$ is the **sigmoid** function.\n",
    "\n",
    "Note that both, $g(x)$ and $\\sigma(x)$ are applied **elementwise**. Further the addition with the bias vector is also applied **elementwise** to each column of the matrix $\\mathbf{X} \\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a)$ Start the implementation of your neural network by defining functions for its different layers $f^{(0)}(x),~f^{(1)}(x),~f^{(2)}(x)$. To do so, please complete the functions skeletons below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implement a linear layer that computes the linear combination $\\mathbf{X} \\mathbf{W}+b$ for a given `input_shape` of $x$ and a given number of hidden units `n_hidden_units`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "# Hint: The following methods might be useful: tf.get_variable(), tf.random_normal_initializer(), tf.matmul(), tf.get_variable()\n",
    "def linear_layer(input_shape, n_hidden_units, x):\n",
    "    \"\"\"\n",
    "    Define a linear layer for your neural network.\n",
    "    :param input_shape: The shape of the input for the layer. Be careful to consider that your data comes in batches. \n",
    "    :param n_hidden: The number of hidden units.\n",
    "    :param x: The input to the layer.\n",
    "    :return: A tuple where the first element is the linear combination of the input with the weights and the biases of the linear layer and the second element is the shape of the output matrix.\n",
    "    \"\"\"\n",
    "    # Create variable named \"weights\".\n",
    "   \n",
    "    weights = tf.get_variable(\"weights\", [input_shape[1], n_hidden_units], dtype=tf.float32, initializer=tf.random_normal_initializer())\n",
    "    #weights = tf.get_variable(\"weights\", [n_hidden_units, 1], tf.constant_initializer(1.0))\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", n_hidden_units, dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "        #initializer=tf.constant_initializer(0.0))\n",
    "    linear_model = tf.matmul(x, weights) + biases\n",
    "    #next_layer_shape = [x.shape[1], n_hidden_units]\n",
    "    next_layer_shape = linear_model.shape\n",
    "    \n",
    "    return linear_model, next_layer_shape\n",
    "    #pass\n",
    "input_shape = (None, 2)\n",
    "print (input_shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function that computes the ReLU nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Define the ReLU activation function.\n",
    "    :param x: The input to the activation function.\n",
    "    :return: The output of the activation function.\n",
    "    \"\"\"\n",
    "    return  tf.nn.relu(x)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function that computes the sigmoid nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Define the sigmoid activation function.\n",
    "    :param x: The input to the activation function.\n",
    "    :return: The output of the activation function.\n",
    "    \"\"\"\n",
    "    return tf.sigmoid(x)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional to the layers above, you need to implement a layer that computes to loss between the prediction of your model and the actual label of your data. For the **loss function** of the network we are going to use the cross entropy between the model logits and the true labels. Take a look at the TensorFlow [API](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits) for further details. You also might want to take a look at page 128 in the Deep Learning Book for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def cross_entropy_loss(labels, unscaled_logits):\n",
    "    \"\"\"\n",
    "    Define the cross entropy loss function between the true labels and the model predictions. Be careful to consider that your data comes in batches.\n",
    "    :param labels: The true labels.\n",
    "    :param unscaled_logits: The final activations produced by your model.\n",
    "    :return: The loss value (a scalar).\n",
    "    \"\"\"\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits= unscaled_logits)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the layers above, you can now define a function that specifies the forward pass through your network. Please make sure that the first hidden layer has $128$ hidden units and the output layer has $1$ hidden unit. **Note**: Make sure that the forward pass does not compute the final prediction of your network, i.e the value returned by the function below should be used as the input to the *sigmoid* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_units = 128\n",
    "\n",
    "# TODO: implement the function below. This is where you build up your model architecture using the layers defined above.\n",
    "def forward(x, input_shape):\n",
    "    \"\"\"\n",
    "    Define the forward pass of your network. Be careful to consider that your data comes in batches.\n",
    "    :param x: The input to your model.\n",
    "    :param input_shape: The shape of the input.\n",
    "    :return: The (unscaled) logit computed by the network.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        linear_model, next_layer_shape = linear_layer(input_shape, n_hidden_units, x)\n",
    "        #scope.reuse_variables()\n",
    "        #unscaled_logits, output_shape = linear_layer(next_layer_shape, 1, relu(linear_model))\n",
    "    with tf.variable_scope(\"output_layer\"):\n",
    "        unscaled_logits, output_shape = linear_layer(next_layer_shape, 1, relu(linear_model))\n",
    "    \n",
    "    \n",
    "    return unscaled_logits\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the first part of the forward path of the neural network is defined, we need a function that computes the actual predictions of the model using the activations returned by `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def inference(unscaled_logits):\n",
    "    \"\"\"\n",
    "    Define the predictions computed by your network\n",
    "    :param unscaled_logits: The (unscaled) logit computed by the network.\n",
    "    :return: The prediction of your model on input x.\n",
    "    \"\"\"\n",
    "    return sigmoid(unscaled_logits)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `cross_entropy` layer and the `forward` function defined above, please implement the loss function for your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def loss(x, y, input_shape):\n",
    "    \"\"\"\n",
    "    Define the loss between the prediction of your model and the actual label.\n",
    "    :param x: The input to your model.\n",
    "    :param y: The shape of the input.\n",
    "    :param input_shape: The loss between what your model predicts and the true label.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    logits = forward(x, input_shape)\n",
    "    entropy = cross_entropy_loss(y, logits)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "    #prediction_soft = soft(predictions)\n",
    "    #cross_entropy_loss(labels, predictions)\n",
    "    \n",
    "    return loss\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b)$ Define the computation graph of your model using the functions implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all that you have done was defining functions that return a **symbolic representation** of the functions computed inside your network. Remember that when using TensorFlow you always need to define a computation graph of your model. Now, use the functions implemented above to define the computation graph for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the computational graph for your model\n",
    "input_shape = (None, 2) # each input has 2 features\n",
    "output_shape = (None, 1) # the output of the model is a scalar corresponding to the probability of belong to class 1 or 2.\n",
    "\n",
    "# TODO: Define placeholers for the input and output\n",
    "input_data = tf.placeholder(tf.float32, name=\"input_data\")\n",
    "output_data_true = tf.placeholder(tf.float32, name=\"output_true\")\n",
    "# TODO: Define the model final activations, predictions and loss of the network\n",
    "with tf.variable_scope(\"logits\"):\n",
    "    logits = forward(input_data, input_shape)\n",
    "    prediction = inference(logits)\n",
    "with tf.variable_scope(\"loss\"):\n",
    "    loss = loss(input_data, output_data_true, input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b)$ Now that we have defined a computation graph for our model we need to train it on training data. Hence, define the optimizer for your model and implement the training loop. After each epoch, print the accuracy of your model on the training data. Once training phase is done, evaluate the test accuracy of your model and plot the **decision boundary** of your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: When evaluating the predictions of your model make sure to compute the class labels from the output of the sigmoid layer before comparing them to the ground truth labels. As a threshold use $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Define the gradient descent optimizer\n",
    "learning_rate = 1e-1\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 10 # we train our model for 10 epochs. You can change this value to observe how the accuracy changes.\n",
    "batch_size = 10 # we use a fixed batch size of 10 data points per batch.\n",
    "n = len(X_train)\n",
    "num_batch = int(n / batch_size)\n",
    "\n",
    "#with tf.name_scope('Accuracy'):\n",
    "#    correct_pred  = tf.equal(tf.argmax(logits, 1), tf.argmax(y_train, 1))\n",
    "#    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# Start a Session \n",
    "tf.reshape(input_data, [batch_size, 2])\n",
    "tf.reshape(output_data_true, [batch_size, 1])\n",
    "#output_data_true.reshape(batch_size, 1)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # TODO: Perform training\n",
    "    #predictions_ = []\n",
    "    #total_correct_prediction = 0\n",
    "    #total_accuracy = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_accuracy = 0\n",
    "        for iteration in range(num_batch):\n",
    "            X_batch = X_train[batch_size * iteration : batch_size * (iteration  + 1) , ]\n",
    "            y_batch = y_train[batch_size * iteration : batch_size * (iteration  + 1) , ]\n",
    "            #print (\"epoch : \", epoch, \"  x_batch shape is : \", X_batch.shape)\n",
    "            #print (\"x_batch shape is\")\n",
    "            #print (X_batch.shape)\n",
    "            #X_batch.reshape((batch_size, 2))\n",
    "            #y_batch.reshape((batch_size, 1))\n",
    "            #accuracy1 = sess.run(accuracy, feed_dict={input_data: X_batch, output_data_true: y_batch})\n",
    "            training_op_, logits_= sess.run([training_op, logits], feed_dict={input_data: X_batch, output_data_true: y_batch})\n",
    "            #predictions_ = predictions_ + logits_ \n",
    "            #print (\"logit shape is : \",logits_.shape, \"  y batch shape is : \", y_batch.shape)\n",
    "            pred = inference(logits_)\n",
    "            pred_ = sess.run(pred, feed_dict={input_data: X_batch})\n",
    "            #print (type(pred_))\n",
    "            #print (pred_.shape)\n",
    "            # float to class label\n",
    "            #print (\"prediction before class label\")\n",
    "            #print (pred_)\n",
    "            for i in range (pred_.shape[0]):\n",
    "                if (pred_[i,] > 0.5):\n",
    "                    pred_[i,] = 1\n",
    "                else :\n",
    "                    pred_[i,] = 0\n",
    "            #print (\"prediction after class label\")\n",
    "            #print (pred_)\n",
    "            correct_pred_  = tf.equal(tf.argmax(pred_, 1), tf.argmax(y_batch, 1))\n",
    "            #correct_pred_ = tf.nn.in_top_k(logits_, y_batch, 1)\n",
    "            #print (\"correct pred shape is : \", correct_pred_.shape)\n",
    "            accuracy_ = tf.reduce_mean(tf.cast(correct_pred_, tf.float32))\n",
    "            #print (\"correct_pred shape is : \", correct_pred_.shape)\n",
    "            #print (\"accuracy_  shape is : \", accuracy_.shape)\n",
    "            acc = sess.run(accuracy_, feed_dict={input_data: X_batch})\n",
    "            #print (\"iteration : \", iteration, \"  accuracy is : \", acc)\n",
    "            total_accuracy += acc\n",
    "        print (\"epoch : \", epoch, \"total accuracy : \", total_accuracy)\n",
    "    #print (\"y batch is : \")\n",
    "    #print (y_batch)\n",
    "    #print (\"pred_ are : \")\n",
    "    #print (pred_)\n",
    "    #predictions_arr = np.asarray(predictions_, dtype=np.float32)\n",
    "        #predictions_arr.reshape([predictions_arr.shape[0] * predictions_arr.shape[1],predictions_arr.shape[2]])\n",
    "    #print (\"epoch : \",epoch, \"  pred array is : \", predictions_arr.shape)\n",
    "        #print (\"epoch : \",epoch, \"  train array is : \", y_batch.shape)\n",
    "        #print (y_batch)\n",
    "        #correct_pred  = tf.equal(tf.argmax(logits, 1), tf.argmax(y_batch, 1))\n",
    "        #print (\"correct pred is : \", correct_pred)\n",
    "        #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        #print (\"accuracy is : \", accuracy)\n",
    "        #accuracy_ = sess.run(accuracy, feed_dict={input_data: X_batch, output_data_true: y_batch})\n",
    "        #print (\"epoch : \", epoch, \" accuracy of training data : \", accuracy_ )\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d)$ **Bonus**: Restructure your code in order to use TensorBoard for displaying the computation graph defined by your model. Additonally, use TensorBoard to log the training and test accuracy of your network during training. (See [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard) for an introduction to TensorBoard.) (+ $2.0$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Role of the activation function (3.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise it is useful to have a plot of the decision boundary of your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a)$ Remove the nonlinearity in your TensorFlow model above, then retrain the model and evaluate its accuracy. What do you observe? ($1.0$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b$) With the nonlinearity removed, do the following: Add an additional hidden layer without nonlinearity in the `forward()` function. Retrain the model and evaluate its accuracy. What do you observe? ($1.0$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c$) What is the reason for this behaviour? Can you come up with a proof of the underlying mathematical fact? ($1.0$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as a solution. The naming should include the assignment number and matriculation IDs of all team members in the following format:\n",
    "**assignment-5_matriculation1_matriculation_2_matriculation3.ipynb** (in case of 3 team members). \n",
    "Make sure to keep the order matriculation1_matriculation_2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please, submit your solution to your tutor (with **[NNIA][assignment-5]** in email subject):\n",
    "1. Maksym Andriushchenko s8mmandr@stud.uni-saarland.de\n",
    "2. Marius Mosbach s9msmosb@stud.uni-saarland.de\n",
    "3. Rajarshi Biswas rbisw17@gmail.com\n",
    "4. Marimuthu Kalimuthu s8makali@stud.uni-saarland.de\n",
    "\n",
    "**If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

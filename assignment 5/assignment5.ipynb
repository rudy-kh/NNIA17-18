{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 5: Classification with TensorFlow (deadline: 8 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Practicing Chain-Rule and Backpropagation (5.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following function $$ f(w, b) = \\frac{e^{w+b} w + \\sigma(w) + b}{(\\sigma(b) + \\sigma(w))Â²}$$ with $\\sigma(x) = \\frac{1}{1+e^{-x}}$ and $w, b \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a)$ Compute the partial derivatives of $f(w, b)$ with respect to $w$ and $b$ analytically. ($0.5$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a)$ Compute the partial derivatives of $f(w, b)$ with respect to $w$ and $b$ analytically. ($0.5$ points)\n",
    "\n",
    "$\\frac{\\partial f}{\\partial w} = \\frac{e^{w+b} (w+1) + \\sigma(w)(1-\\sigma(w))}{(\\sigma(b)+\\sigma(w))^2} - \\frac{2(e^{w+b}w+\\sigma(w)+b)\\sigma(w)(1-\\sigma(w))}{(\\sigma(b)+\\sigma(w))^3}$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial b} = \\frac{e^{w+b}w+1}{(\\sigma(b)+\\sigma(w))^2} - \\frac{2(e^{w+b}w+\\sigma(w)+b)\\sigma(b)(1-\\sigma(b))}{(\\sigma(b)+\\sigma(w))^3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b)$ Decompose the function $f(w, b)$ into primitives. Rewrite the function using these primitives. ($0.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# TODO: Write function primitives in python\n",
    "\n",
    "# Example\n",
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "def exponent(x):\n",
    "    return math.e ** x\n",
    "\n",
    "def addition(x,y):\n",
    "    return x+y\n",
    "\n",
    "def multiplication(x,y):\n",
    "    return x * y\n",
    "\n",
    "def inverse(x):\n",
    "    return 1 / x\n",
    "\n",
    "def minus(x):\n",
    "    return -1 * x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return inverse(addition(1, exponent(minus(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c)$ Draw a computation graph for the function $f(w, b)$ using the primitives defined above as nodes. You may want to use the [NetworkX](https://networkx.github.io/) library for this task. You can install it by running `conda install networkx` from within your activated *nnia* conda environment. ($1.0$ point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb8418a518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.DiGraph()\n",
    "pos = {}\n",
    "labels = {}\n",
    "\n",
    "##input layer\n",
    "pos['w'] = (1,1)\n",
    "pos['b'] = (2,1)\n",
    "\n",
    "##second layer\n",
    "pos['sum1'] = (1.5,2)\n",
    "pos['sigma-b'] = (2,2)\n",
    "pos['sigma-w'] = (3,2)\n",
    "pos['sigma-b2'] = (4,2)\n",
    "\n",
    "##third layer\n",
    "pos['exp'] = (1.5,3)\n",
    "pos['sum2'] = (3.5,3)\n",
    "\n",
    "##forth layer\n",
    "pos['sum3'] = (2,4)\n",
    "pos['square'] = (3.5,4)\n",
    "\n",
    "##fifth layer\n",
    "pos['sum4'] = (2.5,5)\n",
    "pos['inverse'] = (3.5,5)\n",
    "\n",
    "##sixth layer\n",
    "pos['mult'] = (3,6)\n",
    "\n",
    "\n",
    "##input layer\n",
    "labels['w'] = 'w'\n",
    "labels['b'] = 'b'\n",
    "\n",
    "##second layer\n",
    "labels['sum1'] = 'sum1'\n",
    "labels['sigma-b'] = 'sigma-b'\n",
    "labels['sigma-w'] = 'sigma-w'\n",
    "labels['sigma-b2'] = 'sigma-b2'\n",
    "\n",
    "##third layer\n",
    "labels['exp'] = 'exp'\n",
    "labels['sum2'] = 'sum2'\n",
    "\n",
    "##forth layer\n",
    "labels['sum3'] = 'sum3'\n",
    "labels['square'] = 'square'\n",
    "\n",
    "##fifth layer\n",
    "labels['sum4'] = 'sum4'\n",
    "labels['inverse'] = 'inverse'\n",
    "\n",
    "##sixth layer\n",
    "labels['mult'] = 'mult'\n",
    "\n",
    "G.add_edge('w','sum1')\n",
    "G.add_edge('b','sum1')\n",
    "\n",
    "G.add_edge('w','sigma-w')\n",
    "G.add_edge('b','sigma-b2')\n",
    "G.add_edge('b','sigma-b')\n",
    "\n",
    "G.add_edge('sigma-w','sum2')\n",
    "G.add_edge('sigma-b2','sum2')\n",
    "\n",
    "G.add_edge('sum2','square')\n",
    "G.add_edge('square','inverse')\n",
    "G.add_edge('inverse','mult')\n",
    "\n",
    "G.add_edge('sum1','exp')\n",
    "G.add_edge('exp','sum3')\n",
    "G.add_edge('sum3','sum4')\n",
    "G.add_edge('sum4','mult')\n",
    "\n",
    "G.add_edge('sigma-b','sum3')\n",
    "\n",
    "G.add_edge('b','sum4')\n",
    "\n",
    "nx.draw_networkx_labels(G,pos,labels, node_color = 'y', node_size = 1000, font_size=8)\n",
    "\n",
    "nx.draw(G, pos, code_color = 'y', node_size = 750)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d)$ Specify $\\frac{\\partial}{\\partial x} p_i$ for each of the primitives $p_i$. ($1.0$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Specifiy derivatives of the primitives in python\n",
    "\n",
    "# Example\n",
    "def dsquare(x):\n",
    "    return 2 * x\n",
    "\n",
    "def dexponent(x):\n",
    "    return math.e ** x\n",
    "\n",
    "def daddition(x,y):\n",
    "    return 1\n",
    "\n",
    "def dmultiplication1(x,y):\n",
    "    return y\n",
    "\n",
    "def dmultiplication2(x,y):\n",
    "    return x\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def dinverse(x):\n",
    "    return -1 * x ** (-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$e)$ Given the inputs $w = 5, b = -3$. Compute the forward pass of your function. You might want to store the intermediate result in a dictionary in order to later use them in backward pass. ($1.0$ point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.096000975148562\n"
     ]
    }
   ],
   "source": [
    "# Compute the forward pass using the primitives defined above\n",
    "def forward(w, b):\n",
    "    # TODO: Implement\n",
    "    result = {}\n",
    "    result['sum1'] = addition(w,b)\n",
    "    result['exp'] = exponent(result['sum1'])\n",
    "    result['sigmoid1'] = sigmoid(b)\n",
    "    result['sum3'] = addition(result['exp'], result['sigmoid1'])\n",
    "    result['sum4'] = addition(result['sum3'], b)\n",
    "    \n",
    "    result['sigmoid3'] = sigmoid(w)\n",
    "    result['sigmoid2'] = sigmoid(b)\n",
    "    result['sum2'] = addition(result['sigmoid3'], result['sigmoid2'])\n",
    "    result['square'] = square(result['sum2'])\n",
    "    result['inverse'] = inverse(result['square'])\n",
    "    \n",
    "    result['f'] = multiplication(result['sum4'], result['inverse'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "w = 5\n",
    "b = -3\n",
    "\n",
    "result = forward(w,b)\n",
    "\n",
    "print(result['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f)$ For the same inputs as above, perform the backward pass in order to compute $\\frac{\\partial}{\\partial w}f~\\mid_{w=5, b=-3}$ and $\\frac{\\partial}{\\partial b}f~\\mid_{w=5, b=-3}$. ($1.0$ point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.819495913317491 1.6117413012847095\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the backward pass using the derivatives of the primitives defined above.\n",
    "def backward(w, b):\n",
    "    # TODO: Implement\n",
    "    backward_result = {}\n",
    "\n",
    "    backward_result['df-dmult'] = 1\n",
    "    backward_result['dmult-sum4'] = dmultiplication1(\n",
    "        result['sum4'], result['inverse'])\n",
    "\n",
    "    backward_result['dsum4-sum3'] = daddition(result['sum3'], b)\n",
    "    backward_result['dsum4-b'] = daddition(result['sum3'], b)\n",
    "\n",
    "    backward_result['dsum3-exp'] = daddition(result['exp'], result['sigmoid1'])\n",
    "    backward_result['dsum3-sigmoid1'] = daddition(\n",
    "        result['exp'], result['sigmoid1'])\n",
    "\n",
    "    backward_result['dsigmoid1-b'] = dsigmoid(b)\n",
    "    backward_result['dexp-sum1'] = dexponent(result['sum1'])\n",
    "    backward_result['dsum1-w'] = daddition(w, b)\n",
    "    backward_result['dsum1-b'] = daddition(w, b)\n",
    "\n",
    "    backward_result['dmult-inverse'] = dmultiplication2(\n",
    "        result['sum4'], result['inverse'])\n",
    "    backward_result['dinverse-square'] = dinverse(result['square'])\n",
    "    backward_result['dsquare-sum2'] = dsquare(result['sum2'])\n",
    "    backward_result['dsum2-sigmoid2'] = dmultiplication1(\n",
    "        result['sigmoid2'], result['sigmoid3'])\n",
    "    backward_result['dsum2-sigmoid3'] = dmultiplication2(\n",
    "        result['sigmoid2'], result['sigmoid3'])\n",
    "\n",
    "    backward_result['dsigmoid2-b'] = dsigmoid(b)\n",
    "    backward_result['dsigmoid3-w'] = dsigmoid(w)\n",
    "    \n",
    "    dfdw = backward_result['df-dmult'] * backward_result['dmult-sum4'] * backward_result['dsum4-sum3'] * backward_result['dsum3-exp'] * backward_result['dexp-sum1'] * backward_result['dsum1-w'] + \\\n",
    "        backward_result['df-dmult'] * backward_result['dmult-inverse'] * backward_result['dinverse-square'] * \\\n",
    "        backward_result['dsquare-sum2'] * \\\n",
    "        backward_result['dsum2-sigmoid3'] * backward_result['dsigmoid3-w']\n",
    "    dfdb = backward_result['df-dmult'] * backward_result['dmult-sum4'] * backward_result['dsum4-sum3'] * backward_result['dsum3-sigmoid1'] * backward_result['dsigmoid1-b'] + backward_result['df-dmult'] * backward_result['dmult-inverse'] * \\\n",
    "        backward_result['dinverse-square'] * backward_result['dsquare-sum2'] * backward_result['dsum2-sigmoid2'] * \\\n",
    "        backward_result['dsigmoid2-b'] + backward_result['df-dmult'] * \\\n",
    "        backward_result['dmult-sum4'] + backward_result['dsum4-b']\n",
    "\n",
    "    return dfdw, dfdb\n",
    "\n",
    "\n",
    "dfdw, dfdb = backward(w, b)\n",
    "print(dfdw, dfdb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification using Feedforward Neural Networks with TensorFlow (12.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a feedforward neural network using TensorFlow. We will then train the neural network on a simple two class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some dummy data using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "c of shape (270, 1) not acceptable as a color sequence for x with size 270, y with size 270",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Not in cache, or unhashable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[0;32m   4049\u001b[0m                 \u001b[1;31m# must be acceptable as PathCollection facecolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4050\u001b[1;33m                 \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4051\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Not in cache, or unhashable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: RGBA sequence should have length 3 or 4",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-840d09b71817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Plot the training data and color the data point according to their class label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Set1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1708\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1709\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1710\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1711\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[0;32m   4053\u001b[0m                 msg = (\"c of shape {0} not acceptable as a color sequence \"\n\u001b[0;32m   4054\u001b[0m                        \"for x with size {1}, y with size {2}\")\n\u001b[1;32m-> 4055\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4056\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4057\u001b[0m             \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# use cmap, norm after collection is created\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: c of shape (270, 1) not acceptable as a color sequence for x with size 270, y with size 270"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npCYMviQ18f+GmWq6NWLIwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bab2b75c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get some data\n",
    "X, y = make_moons(n_samples= 300, noise=0.15, random_state=123)\n",
    "\n",
    "# Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1)\n",
    "y_train = np.reshape(y_train, newshape=(-1, 1))\n",
    "y_test = np.reshape(y_test, newshape=(-1, 1))\n",
    "\n",
    "# Plot the training data and color the data point according to their class label\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "axes.scatter(X_train[: , 0], X_train[: , 1], c=y_train, cmap='Set1')\n",
    "axes.set_title('Training Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following you are allowed to use predefined functions of the TensorFlow API such as `tf.get_variable()`, `tf.nn.relu(x)` or `tf.random_normal_initializer()`. Addtionally you are allowed to use the gradient descent optimizer `tf.train.GradientDescentOptimizer` predefined by TensorFlow. However, you are not allowed to use any predefined layer from the [API](https://www.tensorflow.org/api_docs/python/tf/layers) such as `tf.Layers.Dense` or `tf.contrib.layers.fully_connected`. It is your task to implement these layers yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(x)$ is the so called hidden layer, and $f^{(2)}(x)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that you will implement in this exercise has the following layers:\n",
    "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$ where $b$ is the batch size and $p$ is the number of features.\n",
    "* $f^{(1)}(x) = g(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $b_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **units** of this particular layer. Additonally, $g(x) = \\max{\\{0, x\\}}$ is the so called **rectified linear unit** (*ReLU*) activation function. \n",
    "* $f^{(2)}(x) = \\sigma(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_1} \\in \\mathbb{R}^{u_1,u_2}$, $b_1 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **units** of this particular layer. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{-x}}$ is the **sigmoid** function.\n",
    "\n",
    "Note that both, $g(x)$ and $\\sigma(x)$ are applied **elementwise**. Further the addition with the bias vector is also applied **elementwise** to each column of the matrix $\\mathbf{X} \\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a)$ Start the implementation of your neural network by defining functions for its different layers $f^{(0)}(x),~f^{(1)}(x),~f^{(2)}(x)$. To do so, please complete the functions skeletons below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implement a linear layer that computes the linear combination $\\mathbf{X} \\mathbf{W}+b$ for a given `input_shape` of $x$ and a given number of hidden units `n_hidden_units`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "# Hint: The following methods might be useful: tf.get_variable(), tf.random_normal_initializer(), tf.matmul(), tf.get_variable()\n",
    "def linear_layer(input_shape, n_hidden_units, x):\n",
    "    \"\"\"\n",
    "    Define a linear layer for your neural network.\n",
    "    :param input_shape: The shape of the input for the layer. Be careful to consider that your data comes in batches. \n",
    "    :param n_hidden: The number of hidden units.\n",
    "    :param x: The input to the layer.\n",
    "    :return: A tuple where the first element is the linear combination of the input with the weights and the biases of the linear layer and the second element is the shape of the output matrix.\n",
    "    \"\"\"\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", [input_shape, n_hidden_units],\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", [n_hidden_units, 1],\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    linear_model = tf.matmul(x, weights) + biases\n",
    "    #next_layer_shape = [x.shape[1], n_hidden_units]\n",
    "    next_layer_shape = linear_model.shape\n",
    "    \n",
    "    return linear_model, next_layer_shape\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function that computes the ReLU nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Define the ReLU activation function.\n",
    "    :param x: The input to the activation function.\n",
    "    :return: The output of the activation function.\n",
    "    \"\"\"\n",
    "    return  tf.nn.relu(x)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function that computes the sigmoid nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Define the sigmoid activation function.\n",
    "    :param x: The input to the activation function.\n",
    "    :return: The output of the activation function.\n",
    "    \"\"\"\n",
    "    return tf.sigmoid(x)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional to the layers above, you need to implement a layer that computes to loss between the prediction of your model and the actual label of your data. For the **loss function** of the network we are going to use the cross entropy between the model logits and the true labels. Take a look at the TensorFlow [API](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits) for further details. You also might want to take a look at page 128 in the Deep Learning Book for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def cross_entropy_loss(labels, unscaled_logits):\n",
    "    \"\"\"\n",
    "    Define the cross entropy loss function between the true labels and the model predictions. Be careful to consider that your data comes in batches.\n",
    "    :param labels: The true labels.\n",
    "    :param unscaled_logits: The final activations produced by your model.\n",
    "    :return: The loss value (a scalar).\n",
    "    \"\"\"\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits= unscaled_logits)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the layers above, you can now define a function that specifies the forward pass through your network. Please make sure that the first hidden layer has $128$ hidden units and the output layer has $1$ hidden unit. **Note**: Make sure that the forward pass does not compute the final prediction of your network, i.e the value returned by the function below should be used as the input to the *sigmoid* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_units = 128\n",
    "\n",
    "# TODO: implement the function below. This is where you build up your model architecture using the layers defined above.\n",
    "def forward(x, input_shape):\n",
    "    \"\"\"\n",
    "    Define the forward pass of your network. Be careful to consider that your data comes in batches.\n",
    "    :param x: The input to your model.\n",
    "    :param input_shape: The shape of the input.\n",
    "    :return: The (unscaled) logit computed by the network.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"hidden_layer\"):\n",
    "        linear_model, next_layer_shape = linear_layer(input_shape, n_hidden_units, x)\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(\"output_layer\"):\n",
    "        unscaled_logits, output_shape = linear_layer(next_layer_shape, n_hidden_units, relu(linear_model))\n",
    "    \n",
    "    \n",
    "    return unscaled_logits\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the first part of the forward path of the neural network is defined, we need a function that computes the actual predictions of the model using the activations returned by `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def inference(unscaled_logits):\n",
    "    \"\"\"\n",
    "    Define the predictions computed by your network\n",
    "    :param unscaled_logits: The (unscaled) logit computed by the network.\n",
    "    :return: The prediction of your model on input x.\n",
    "    \"\"\"\n",
    "    return sigmoid(unscaled_logits)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `cross_entropy` layer and the `forward` function defined above, please implement the loss function for your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the function below.\n",
    "def loss(x, y, input_shape):\n",
    "    \"\"\"\n",
    "    Define the loss between the prediction of your model and the actual label.\n",
    "    :param x: The input to your model.\n",
    "    :param y: The shape of the input.\n",
    "    :param input_shape: The loss between what your model predicts and the true label.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    logits = forward(x, input_shape)\n",
    "    entropy = cross_entropy_loss(labels, logits)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "    #prediction_soft = soft(predictions)\n",
    "    #cross_entropy_loss(labels, predictions)\n",
    "    \n",
    "    return loss\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b)$ Define the computation graph of your model using the functions implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all that you have done was defining functions that return a **symbolic representation** of the functions computed inside your network. Remember that when using TensorFlow you always need to define a computation graph of your model. Now, use the functions implemented above to define the computation graph for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7465c5f58b6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moutput_data_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"output_true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# TODO: Define the model final activations, predictions and loss of the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-cd095d29838c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(x, input_shape)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hidden_layer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mlinear_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_layer_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-e5af75456e03>\u001b[0m in \u001b[0;36mlinear_layer\u001b[1;34m(input_shape, n_hidden_units, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Create variable named \"weights\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     weights = tf.get_variable(\"weights\", [input_shape, n_hidden_units],\n\u001b[1;32m---> 13\u001b[1;33m         initializer=tf.random_normal_initializer())\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Create variable named \"biases\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     biases = tf.get_variable(\"biases\", [n_hidden_units, 1],\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m           use_resource=use_resource)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[0mshould_check\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreuse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    432\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     30\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n\u001b[0;32m     34\u001b[0m           self._value != value):\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "# Define the computational graph for your model\n",
    "input_shape = (None, 2) # each input has 2 features\n",
    "output_shape = (None, 1) # the output of the model is a scalar corresponding to the probability of belong to class 1 or 2.\n",
    "\n",
    "# TODO: Define placeholers for the input and output\n",
    "input_data = tf.placeholder(tf.float32, shape=input_shape, name=\"input_data\")\n",
    "output_data_true = tf.placeholder(tf.float32, shape=output_shape, name=\"output_true\")\n",
    "# TODO: Define the model final activations, predictions and loss of the network\n",
    "logits = forward(input_data, input_shape)\n",
    "prediction = inference(logits)\n",
    "loss = loss(input_data, output_data, input_shape)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b)$ Now that we have defined a computation graph for our model we need to train it on training data. Hence, define the optimizer for your model and implement the training loop. After each epoch, print the accuracy of your model on the training data. Once training phase is done, evaluate the test accuracy of your model and plot the **decision boundary** of your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: When evaluating the predictions of your model make sure to compute the class labels from the output of the sigmoid layer before comparing them to the ground truth labels. As a threshold use $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Define the gradient descent optimizer\n",
    "learning_rate = 1e-1\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 10 # we train our model for 10 epochs. You can change this value to observe how the accuracy changes.\n",
    "batch_size = 10 # we use a fixed batch size of 10 data points per batch.\n",
    "n = len(X_train)\n",
    "num_batch = int(n / batch_size)\n",
    "\n",
    "# Start a Session \n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # TODO: Perform training\n",
    "    for epoch in range(epochs):\n",
    "        for iteration in range(num_batch):\n",
    "            X_batch = X_train[batch_size * iteration, batch_size * (iteration  + 1)]\n",
    "            Y_batch = Y_train[batch_size * iteration, batch_size * (iteration  + 1)]\n",
    "            sess.run(training_op, feed_dict={input_data: X_batch, output_data_true: y_batch})\n",
    "        #acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        #print(epoch, \"Batch accuracy:\", acc_train)\n",
    "        \n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_train, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))    \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d)$ **Bonus**: Restructure your code in order to use TensorBoard for displaying the computation graph defined by your model. Additonally, use TensorBoard to log the training and test accuracy of your network during training. (See [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard) for an introduction to TensorBoard.) (+ $2.0$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Role of the activation function (3.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise it is useful to have a plot of the decision boundary of your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a)$ Remove the nonlinearity in your TensorFlow model above, then retrain the model and evaluate its accuracy. What do you observe? ($1.0$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b$) With the nonlinearity removed, do the following: Add an additional hidden layer without nonlinearity in the `forward()` function. Retrain the model and evaluate its accuracy. What do you observe? ($1.0$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c$) What is the reason for this behaviour? Can you come up with a proof of the underlying mathematical fact? ($1.0$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as a solution. The naming should include the assignment number and matriculation IDs of all team members in the following format:\n",
    "**assignment-5_matriculation1_matriculation_2_matriculation3.ipynb** (in case of 3 team members). \n",
    "Make sure to keep the order matriculation1_matriculation_2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please, submit your solution to your tutor (with **[NNIA][assignment-5]** in email subject):\n",
    "1. Maksym Andriushchenko s8mmandr@stud.uni-saarland.de\n",
    "2. Marius Mosbach s9msmosb@stud.uni-saarland.de\n",
    "3. Rajarshi Biswas rbisw17@gmail.com\n",
    "4. Marimuthu Kalimuthu s8makali@stud.uni-saarland.de\n",
    "\n",
    "**If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
